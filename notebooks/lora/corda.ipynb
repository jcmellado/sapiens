{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f50ca7-2d9d-42a0-81ed-da1b7c8b06f0",
   "metadata": {},
   "source": [
    "# CorDA\n",
    "\n",
    "Revisión informal de \"_CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning_\".\n",
    "\n",
    "- [https://arxiv.org/pdf/2406.05223](https://arxiv.org/pdf/2406.05223)\n",
    "\n",
    "Técnica publicada en junio de 2024 que propone un método alternativo para inicializar las matrices de bajo rango utilizadas en _LoRA_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c47aa0-adef-4a18-87b8-06e85d87cc92",
   "metadata": {},
   "source": [
    "## 1. Arquitectura\n",
    "\n",
    "_LoRA_ utiliza matrices de bajo rango para añadir nuevo conocimiento a un modelo preentrenado sin modificar los pesos aprendidos.\n",
    "\n",
    "```\n",
    "Weights x Input + (Weights' x Input) ─> Output\n",
    "```\n",
    "\n",
    "_CorDA_ propone inicializar las matrices de bajo rango a partir de las activaciones internas del modelo para un conjunto de muestras, y así orientar el reentrenamiento según el contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19379d6-3bfd-4b8d-a0f5-4d4be8bbd1c6",
   "metadata": {},
   "source": [
    "## 2. Análisis\n",
    "\n",
    "Revisa el reentrenamiento de modelos usando _LoRA_, argumenta que su inicialización aleatoria no considera ni el contexto de la nueva tarea ni la estructura de conocimiento del modelo preentrenado, y propone una alternativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb3cb9-ddb0-446e-ad96-07998d5f9ea5",
   "metadata": {},
   "source": [
    "### 2.1. Low-Rank Adaptation (LoRA)\n",
    "\n",
    "_LoRA_ se basa en la premisa de que reentrenar un modelo con nuevo conocimiento requiere un cambio en los pesos que se encuentra en un subespacio de mucha menor dimensión que el espacio donde se encuentran los pesos aprendidos por el modelo.\n",
    "\n",
    "- [https://github.com/jcmellado/sapiens/blob/main/notebooks/lora/lora.ipynb](https://github.com/jcmellado/sapiens/blob/main/notebooks/lora/lora.ipynb)\n",
    "\n",
    "Lo que permite expresar dicho cambio de pesos como un producto de dos matrices de bajo rango.\n",
    "\n",
    "Durante el reentrenamiento los pesos aprendidos del modelo se congelan, y se aprenden sólo las matrices de bajo rango, considerablemente más pequeñas.\n",
    "\n",
    "- $ W^{*} = W + \\Delta{W} = W + BA $\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $W^{*}$: los pesos después del reentrenamiento.\n",
    "\n",
    "- $W$: los pesos originales del modelo, $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$.\n",
    "\n",
    "- $\\Delta{W}$: el cambio de pesos debido al reentrenamiento.\n",
    "\n",
    "- $BA$: la descomposición de $\\Delta{W}$ en las matrices $B \\in \\mathbb{R}^{d_{out} \\times r}$ y $A \\in \\mathbb{R}^{r \\times d_{in}}$ con rango $r \\ll \\operatorname{min}(d_{out}, d_{in})$.\n",
    "\n",
    "La matriz $A$ se inicializa habitualmente utilizando el método de _Kaiming_ (distribución normal de media cero y desviación típica $\\sqrt{2 / d_{in}}$), y la matriz $B$ con ceros. Al ser cero la segunda matriz, el cambio de pesos inicial es cero, y el reentrenamiento empieza con los valores originales de los pesos aprendidos por el modelo.\n",
    "\n",
    "La ventaja de esta técnica es que no requiere reentrenar el modelo completo, y el cambio de pesos aprendido se puede sumar directamente a los pesos del modelo durante la inferencia para añadir el nuevo conocimiento sin añadir latencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76726612-05f3-4dd7-9721-fbcaf67b633c",
   "metadata": {},
   "source": [
    "### 2.2. Covariance Matrix\n",
    "\n",
    "_CorDA_ se apoya en la _matriz de covarianza_, por lo que es conveniente hacer una introducción informal a este concepto.\n",
    "\n",
    "La _covarianza_ es un valor escalar que indica si dos variables tienden a variar en la misma dirección (covarianza positiva) o en direcciones opuestas (covarianza negativa).\n",
    "\n",
    "Se calcula promediando la suma del producto de las desviaciones de cada variable respecto a su media.\n",
    "\n",
    "- $ \\operatorname{Cov}(X, Y) = \\dfrac{1}{{n-1}} \\sum\\limits_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y}) $\n",
    "\n",
    "La _matriz de covarianza_ es una matriz cuadrada que contiene las covarianzas calculadas para todas las posibles combinaciones de pares de variables.\n",
    "\n",
    "En la diagonal principal se sitúan las varianzas (covarianza de una variable consigo misma), y en el resto de posiciones las covarianzas entre las distintas combinaciones de variables.\n",
    "\n",
    "- $ \\begin{bmatrix}\n",
    "\\operatorname{Var}(X) & \\operatorname{Cov}(X, Y)\n",
    "\\\\ \\operatorname{Cov}(Y, X) & \\operatorname{Var}(Y)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Este resultado se aplica a redes neuronales, por ejemplo para conocer la estructura de correlaciones en las activaciones de salida de una capa. Lo que se puede calcular a través de la matriz de covarianza $\\Sigma_y$ de la salida $\\mathbf{y}$, para una entrada $\\mathbf{x}$ de media $\\mu_x$ y matriz de covarianza $\\Sigma_x$.\n",
    "\n",
    "- $ \\Sigma_y = \\operatorname{Cov}(\\mathbf{y}) = \\operatorname{Cov}(\\mathbf{W} \\mathbf{x} + \\mathbf{b}) = \\mathbf{W} \\Sigma_x \\mathbf{W}^T $\n",
    "\n",
    "La matriz $b$ de _bias_ es una constante y no influye, y el paso final se alcanza por un resultado conocido de la covarianza, si $\\mathbf{y} = \\mathbf{A} \\mathbf{x}$, entonces $\\operatorname{Cov}(\\mathbf{y}) = \\mathbf{A} \\operatorname{Cov}(\\mathbf{x}) \\mathbf{A}^T$.\n",
    "\n",
    "Es decir, que si se tiene la matriz de covarianza de la entrada, y los pesos, entonces se puede calcular la matriz de covarianza de la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe74d31-4824-4731-829c-a3dc9fea0111",
   "metadata": {},
   "source": [
    "### 2.3. Inner Product\n",
    "\n",
    "Aunque el _paper_ menciona la matriz de covarianza, lo que realmente se calcula es el _producto interno_ de una matriz por su traspuesta.\n",
    "\n",
    "- $ X X^T $\n",
    "\n",
    "Esta expresión no se corresponde exactamente con la expresión de cálculo de la covarianza, donde se restan las medias de los valores, pero es proporcional si se considera que los valores ya están centrados en una media $\\mu$ próxima a cero.\n",
    "\n",
    "- $ (X - \\mu) (X - \\mu)^T $\n",
    "\n",
    "Lo único que falta es promediar, pero no hacerlo quiere decir simplemente que el resultado no está normalizado por $n - 1$.\n",
    "\n",
    "La matriz resultante es similar a la de covarianza, pero no está centrada ni promediada.\n",
    "\n",
    "Conserva las características internas relevantes y es computacionalmente más eficiente de calcular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4020698-5fd6-4a34-8abe-56f8e99aa04d",
   "metadata": {},
   "source": [
    "### 2.4. Singular Value Decomposition (SVD)\n",
    "\n",
    "_CorDA_ se apoya en la _Descomposición en Valores Singulares_ (_SVD_), por lo que es conveniente realizar también una introducción informal a esta técnica.\n",
    "\n",
    "_SVD_ descompone una matriz en tres matrices.\n",
    "\n",
    "- $ A = U \\Sigma V^T $\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$: la matriz que se quiere descomponer.\n",
    "\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$: una matriz ortogonal (formada por vectores unitarios y perpendiculares entre sí).\n",
    "\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$: una matriz diagonal.\n",
    "\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$: una matriz ortogonal.\n",
    "\n",
    "Los vectores columnas de $U$ se llaman vectores singulares _izquierdos_, y los de $V$ se llaman vectores singulares _derechos_.\n",
    "\n",
    "La matriz $\\Sigma$ es una matriz diagonal que contiene los valores singulares de la matriz $A$ en orden descendente. Estos valores son las raíces cuadradas de los autovalores de $A^TA$. Siendo los _valores singulares_ un conjunto de escalares que indican como se escala un vector al multiplicarse por la matriz. Y los _autovalores_ escalares que miden cómo una matriz estira o comprime ciertos vectores. \n",
    "\n",
    "Si se considera que la matriz $A$ representa una transformación lineal aplicada sobre un conjunto de datos. La matriz $V^T$ gira los vectores originales de entrada, manteniendo los ángulos originales. La matriz $\\Sigma$ escala los vectores, cambiando su magnitud. Y la matriz $U$ realiza otra rotación para llevar los vectores a su posición final, manteniendo aún los ángulos originales.\n",
    "\n",
    "_SVD_ descompone una matriz en sus componentes principales, revelando la estructura interna de la matriz y las operaciones que realiza al aplicarla sobre un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb309f2-785f-4edc-8fc2-a0af410a18fc",
   "metadata": {},
   "source": [
    "### 2.5. Context-Oriented Decomposition\n",
    "\n",
    "El argumento principal de _CorDA_ es que la inicialización aleatoria de _LoRA_ no tiene en cuenta ni el contexto de la nueva tarea ni la estructura de conocimiento del modelo. _CorDA_ propone inicializar las matrices de forma que se respeten los patrones de activación aprendidos por el modelo.\n",
    "\n",
    "Toma un conjunto de muestras del _dataset_ de entrenamiento para la tarea específica que se quiere reentrenar (pares preguntas-respuestas, escritura de código, resolución de problemas matemáticos, etc). Realiza la inferencia con los pesos aprendidos del modelo, sin modificar. Y obtiene los valores de las activaciones a las entradas de las capas lineales del modelo.\n",
    "\n",
    "A continuación calcula la matriz de covarianza de las activaciones en la entrada de cada capa lineal del modelo.\n",
    "\n",
    "- $ C = X X^T \\in \\mathbb{R}^{d_{in} \\times d_{in}}$\n",
    "\n",
    "Y aplica _SVD_ al producto de la matriz de pesos de la capa lineal por la matriz de covarianza.\n",
    "\n",
    "- $ \\operatorname{SVD}(WC) = U \\Sigma V^T $\n",
    "\n",
    "Siendo:\n",
    " \n",
    "- $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$: la matriz de pesos de la capa lineal.\n",
    "\n",
    "- $C \\in \\mathbb{R}^{d_{in} \\times d_{in}}$: la matriz de covarianza.\n",
    "\n",
    "- $U \\in \\mathbb{R}^{d_{out} \\times d_{out}}$: la matriz ortogonal con los vectores singulares izquierdos. \n",
    "\n",
    "- $\\Sigma \\in \\mathbb{R}^{d_{out} \\times d_{in}}$: la matriz diagonal con los valores singulares en orden descendente en su diagonal principal.\n",
    "\n",
    "- $V \\in \\mathbb{R}^{d_{in} \\times d_{in}}$: la matriz ortogonal con los vectores singulares derechos.\n",
    "\n",
    "Expresión que puede desarrollarse haciendo la multiplicación de las matrices.\n",
    "\n",
    "- $ U \\Sigma V^T = \\displaystyle \\sum\\limits_{i=1}^R \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T $\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $\\sigma_i$: los valores singulares.\n",
    "\n",
    "- $\\mathbf{u}_i \\in \\mathbb{R}^{d_{out}}$: los vectores singulares izquierdos.\n",
    "\n",
    "- $\\mathbf{v}_i \\in \\mathbb{R}^{d_{in}}$: los vectores singulares derechos.\n",
    "\n",
    "- $R \\le \\min(d_{out}, d_{in})$: el rango (número de valores singulares distintos de cero) de $WC$.\n",
    "\n",
    "Calcular la matriz de covarianza sobre las activaciones de entrada significa obtener la correlación entre las distintas dimensiones de activación a lo largo de todo el conjunto de muestras. Es decir, la matriz $C$ captura cómo las diferentes características de entrada tienden a variar juntas en el contexto de una tarea.\n",
    "\n",
    "Aplicar _SVD_ a los pesos multiplicados por la matriz de covarianza guía la descomposición de la matriz de pesos teniendo en cuenta la correlación entre las dimensiones de los _embeddings_. Orienta el proceso de factorización a través del contexto representativo de las muestras tomadas y la tarea específica realizada por el modelo para dichas muestras.\n",
    "\n",
    "Los componentes $\\mathbf{u}_i$ y $\\mathbf{v}_i$ con los valores singulares $\\sigma_i$ de mayor valor representan las características dominantes de la tarea asociada a $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1da379-6f96-40b6-a4ac-65b9c46e8e96",
   "metadata": {},
   "source": [
    "### 2.6. Fine-Tuning\n",
    "\n",
    "_CorDA_ calcula unos nuevos pesos antes de realizar el _fine-tuning_ multiplicando el resultado de la descomposición con _SVD_ por la inversa de la matriz de covarianza.\n",
    "\n",
    "- $ \\hat{W} = \\operatorname{SVD}(W C) C^{-1} = U \\Sigma (V^T C^{-1}) = \\displaystyle \\sum\\limits_{i=1}^{R} \\sigma_i \\mathbf{u}_i \\mathbf{\\hat{v}}_i^T $\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $\\hat{W}$: los pesos inicializados para realizar el _fine-tuning_.\n",
    "\n",
    "- $C^{-1}$: la inversa de la matriz de covarianza.\n",
    "\n",
    "- $\\mathbf{\\hat{v}}_i^T$: el vector $i$-ésimo de la matriz $V^T C^{-1}$.\n",
    "\n",
    "Si la matriz $C$ no es invertible, se aplica un proceso iterativo para hacerla invertible. Se suma a la diagonal de $C$ un valor positivo, calculado como un coeficiente multiplicado por la media de la diagonal. Y se repite la operación, doblando el coeficiente en cada paso, hasta que la distancia L2 entre $CC^{-1}$ y la matriz identidad es inferior a un umbral.\n",
    "\n",
    "Esta inicialización preserva los valores originales de los pesos entrenados por el modelo al iniciar el reentrenamiento, de forma similar a lo que hace _LoRA_ inicializando con ceros.\n",
    "\n",
    "Mientras que _LoRA_ cambia las capas para sumar a los pesos originales las matrices de bajo rango. Los pesos originales son congelados, y las matrices de bajo rango aprendidas.\n",
    "\n",
    "- $ W^{*} = W + BA $\n",
    "\n",
    "_CorDA_ calcula nuevos pesos a partir de los originales y las matrices de bajo rango inicializadas.\n",
    "\n",
    "- $ W' = W - BA $\n",
    "\n",
    "Y cambia las capas para utilizar los nuevos pesos y sumar las matrices de bajo rango. Los nuevos pesos son congelados, y las matrices de bajo rango aprendidas.\n",
    "\n",
    "- $ W^{*} = W' + BA $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a33439-4917-4701-873b-c544887a5928",
   "metadata": {},
   "source": [
    "### 2.7. Knowledge-Preserved Adaptation\n",
    "\n",
    "Para preservar el conocimiento general aprendido por un modelo durante el _fine-tuning_, _CorDA_ propone entrenar los últimos $r$ componentes con los valores singulares de menor valor, y congelar el resto de componentes.\n",
    "\n",
    "- $ B = U_{[:,-r:]} \\sqrt{\\Sigma}_{[-r:]} $\n",
    "\n",
    "- $ A = \\sqrt{\\Sigma}_{[-r:]} (V^T C^{-1})_{[-r:,:]} $\n",
    "\n",
    "- $ BA = \\displaystyle \\sum\\limits_{i=R-r+1}^R \\sigma_i \\mathbf{u}_i \\mathbf{\\hat{v}}_i^T $\n",
    "\n",
    "Siendo $U_{[:,-r:]}$, $\\sqrt{\\Sigma}_{[-r:]}$, y $(V^T C^{-1})_{[-r:,:]}$, las últimas $r$ columnas de $U$, los últimos $r$ elementos de la diagonal de $\\Sigma$, y las últimas $r$ filas de $V^T C^{-1}$, respectivamente.\n",
    "\n",
    "La idea es que si se crea la matriz de covarianza con muestras de tipo pregunta-respuesta, los componentes con los valores singulares de mayor valor representan las características dominantes de este tipo de tareas donde se enfatiza el conocimiento general del modelo. Por lo tanto, entrenando los componentes con valores singulares de menor valor, y congelando los de mayor valor, se preserva el conocimiento general del modelo.\n",
    "\n",
    "Congelar los de mayor valor quiere decir los primeros $R-r$ componentes de $W'$ se congelan y el resto se entrenan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf6907-c449-49ea-b141-c11c17007d48",
   "metadata": {},
   "source": [
    "### 2.8. Instruction-Previewed Adaptation\n",
    "\n",
    "Para favorecer el _fine-tuning_ de una tarea, _CorDA_ propone entrenar los primeros $r$ componentes con los valores singulares de mayor valor, y congelar el resto de componentes.\n",
    "\n",
    "- $ B = U_{[:,:r]} \\sqrt{\\Sigma}_{[:r]} $\n",
    "\n",
    "- $ A = \\sqrt{\\Sigma}_{[:r]} (V^T C^{-1})_{[:r,:]} $\n",
    "\n",
    "- $ BA = \\displaystyle \\sum\\limits_{i=1}^r \\sigma_i \\mathbf{u}_i \\mathbf{\\hat{v}}_i^T $\n",
    "\n",
    "Siendo $U_{[:,:r]}$, $\\sqrt{\\Sigma}_{[:r]}$, y $(V^T C^{-1})_{[:r,:]}$, las primeras $r$ columnas de $U$, los primeros $r$ elementos de la diagonal de $\\Sigma$, y las primeras $r$ filas de $V^T C^{-1}$, respectivamente.\n",
    "\n",
    "La idea es que si se crea la matriz de covarianza con muestras de problemas de escritura de código, los componentes con los valores singulares de mayor valor representan las características dominantes de este tipo de tareas donde se enfatiza la capacidad del modelo de realizar esa tarea específica. Por lo tanto, entrenando los componentes con valores singulares de mayor valor, y congelando los de menor valor, se enfatiza el entrenamiento de la capacidad de realizar la tarea por parte del modelo.\n",
    "\n",
    "Congelar los de menor valor quiere decir que los últimos $R-r$ componentes de $W'$ se congelan y el resto se entrenan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
