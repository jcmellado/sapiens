{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254fffb7-fbbf-41a6-81a8-c1a11bb1d5cb",
   "metadata": {},
   "source": [
    "# WINA\n",
    "\n",
    "Revisión informal de _WINA_ (_Weight Informed Neuron Activation for Accelerating Large Language Model Inference_).\n",
    "\n",
    "Técnica publicada por Microsoft en mayo de 2025.\n",
    "\n",
    "- [https://arxiv.org/pdf/2505.19427](https://arxiv.org/pdf/2505.19427)\n",
    "\n",
    "Desactiva neuronas de manera selectiva (_sparse activation_), sin necesidad de reentrenar los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddfe40-018a-4b77-8927-007417ff962c",
   "metadata": {},
   "source": [
    "## 1. Arquitectura\n",
    "\n",
    "Las técnicas habituales utilizan únicamente las magnitudes de las entradas para decidir qué neuronas desactivar.\n",
    "\n",
    "```\n",
    "Input ─> Gate ─> Layer ─> Output\n",
    "```\n",
    "\n",
    "_WINA_ utiliza las magnitudes de las entradas y de los pesos.\n",
    "\n",
    "```\n",
    "Input ─> Gate ─> Layer ─> Output\n",
    "          ↑        ↓\n",
    "          └──── Weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196a787-106d-4fe8-b588-7b11df4cee4d",
   "metadata": {},
   "source": [
    "## 2. Análisis\n",
    "\n",
    "Propone una expresión muy sencilla para la función de puerta (_gate_) de las capas, y demuestra matemáticamente su superioridad frente a otros métodos, pero impone condiciones a la hora de evaluar los modelos que deben conocerse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2e1d4-890a-4da8-a894-98cc2934d6dd",
   "metadata": {},
   "source": [
    "### 2.1. Función Objetivo\n",
    "\n",
    "El marco teórico del _paper_ parte de la definición de los elementos que componen una capa y la operación básica que realiza.\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $W \\in \\mathbb{R}^{m \\times n}$ la matriz de pesos de la capa.\n",
    "\n",
    "- $x \\in \\mathbb{R}^{n}$ un vector arbitrario de entrada a la capa.\n",
    "\n",
    "- $y \\leftarrow W x$ la transformación lineal estándar que multiplica pesos por entrada para obtener la salida.\n",
    "\n",
    "Plantea el problema de desactivar neuronas como aplicar una máscara binaria sobre la entrada.\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $m \\in \\{0, 1\\}^n$ el vector que enmascara la entrada.\n",
    "\n",
    "- $y_m \\leftarrow W (m \\odot x)$ la salida de la capa aplicando la máscara $m$ a la entrada.\n",
    "\n",
    "Y define el objetivo como encontrar la máscara $m$ que minimice la diferencia entre la salida original y la nueva aplicando dicha máscara.\n",
    "\n",
    "- $ \\underset{m \\in \\{0, 1\\}^n}{\\text{arg min}} \\lVert W x - W (m \\odot x) \\rVert^2_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da032ff-2ab9-46c2-b006-a02badaf7c54",
   "metadata": {},
   "source": [
    "### 2.2. Binary Activation Gate\n",
    "\n",
    "Los elementos $m_i$ de la máscara se calculan utilizando la entrada y los pesos.\n",
    "\n",
    "-  $ m_i = \\begin{cases} 1 \\quad \\text{si} \\mid x_i c_i \\mid \\text{está entre los top-}K \\text{valores en} \\mid x \\odot \\ c \\mid \\\\ 0 \\quad \\text{en cualquier otro caso} \\end{cases} $\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $c$ el vector resultante de calcular la norma L2 columna a columna sobre la matriz de pesos.\n",
    "\n",
    "- $c_i = \\lVert W_{\\cdot,i} \\rVert_2$ el elemento $i$-ésimo resultante de calcular la norma L2 sobre la columna $i$ de la matriz de pesos.\n",
    "\n",
    "- $\\mid x \\odot c \\mid$ el producto de la entrada por las normas, elemento a elemento, en valor absoluto.\n",
    "\n",
    "- $\\mid x_i c_i \\mid$ el elemento $i$-ésimo del vector de los productos de la entrada por las normas.\n",
    "\n",
    "- $\\text{top-}K$ el subconjunto de los $K$ valores mayores del vector $\\mid x \\odot c \\mid$.\n",
    "\n",
    "- $K$ un valor escalar fijo que define el número de entradas que se deben dejar pasar.\n",
    "\n",
    "$K$ puede tener el mismo valor para todas las capas, o definirse de forma más específica para cada capa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17baf6e-a6f4-489d-8390-a0c1076a8404",
   "metadata": {},
   "source": [
    "### 2.3. Single Layer\n",
    "\n",
    "Para demostrar las bondades de _WINA_, se compara con _TEAL_, otra técnica similar de referencia, que sólo tiene en cuenta los valores absolutos del vector de entrada.\n",
    "\n",
    "El _paper_ establece que si la matriz de pesos tiene columnas ortonormales.\n",
    "\n",
    "- $ W^T W = I_n $\n",
    "\n",
    "Entonces se verifica que el error cometido por _WINA_ es menor o igual que el cometido por _TEAL_ para cualquier nivel $K < n$ de activación deseado.\n",
    "\n",
    "- $ \\mathbb{E} \\left[ \\lVert W x_{\\text{WINA}} - W x \\rVert^2_2 \\right] \\le \\mathbb{E} \\left[ \\lVert W x_{\\text{TEAL}} - W x \\rVert^2_2 \\right] $\n",
    "\n",
    "Donde $W x_{\\text{WINA}}$ es la salida de la capa con _WINA_, y $W x_{\\text{TEAL}}$ la salida con _TEAL_.\n",
    "\n",
    "En los apéndices del _paper_ puede encontrarse la demostración formal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513c149-6491-4f40-bede-eecd8983cf97",
   "metadata": {},
   "source": [
    "### 2.4. Multiple Layers\n",
    "\n",
    "Suponiendo que todas las matrices de pesos de una red, con cualquier número de capas, tienen columnas ortonormales.\n",
    "\n",
    "Se verifica que el error cometido por _WINA_ es menor o igual que el cometido por _TEAL_ a la salida de la red para cualquier nivel $K_i < n$ de activación deseado.\n",
    "\n",
    "- $ \\mathbb{E} \\left[ \\lVert y_{\\text{WINA}} - y \\rVert^2_2 \\right] \\le \\mathbb{E} \\left[ \\lVert y_{\\text{TEAL}} - y \\rVert^2_2 \\right] $\n",
    "\n",
    "Donde $y_{\\text{WINA}}$ es el resultado obtenido por el modelo con _WINA_, e $y_{\\text{TEAL}}$ el obtenido con _TEAL_.\n",
    "\n",
    "En los apéndices del _paper_ puede encontrarse la demostración formal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282bf5f-3d31-405d-b0f2-6cf58e121ff7",
   "metadata": {},
   "source": [
    "### 2.5. Activation Layer\n",
    "\n",
    "Establece que si la matriz de pesos de una capa con una función de activación tiene columnas ortonormales.\n",
    "\n",
    "Y la función de activación es monotónica no decreciente, es decir, que su salida no disminuye cuando aumenta su entrada.\n",
    "\n",
    "- $f: \\mathbb{R} \\rightarrow \\mathbb{R}, \\quad \\forall x_1 \\le x_2 \\quad f(x_1) \\le f(x_2) $\n",
    "\n",
    "Entonces se verifica que el error cometido por _WINA_ es menor o igual que el cometido por _TEAL_ para cualquier nivel $K < n$ de activación deseado.\n",
    "\n",
    "- $ \\mathbb{E} \\left[ \\lVert f(W x_{\\text{WINA}}) - f(W x) \\rVert^2_2 \\right] \\le \\mathbb{E} \\left[ \\lVert f(W x_{\\text{TEAL}}) - f(W x) \\rVert^2_2 \\right] $\n",
    "\n",
    "Ejemplos de este tipo de funciones de activación son _ReLU_, _LeakyReLU_, y cualquiera de sus variantes.\n",
    "\n",
    "En los apéndices del _paper_ puede encontrarse la demostración formal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278c0b2-7cca-4b9c-b4b6-84459f43f664",
   "metadata": {},
   "source": [
    "### 2.6. Multiple Activation Layers\n",
    "\n",
    "Suponiendo que todas las matrices de pesos de una red, con cualquier número de capas, tienen columnas ortonormales.\n",
    "\n",
    "Y todas las funciones de activación son monotónicas no decrecientes.\n",
    "\n",
    "Se verifica que el error cometido por _WINA_ es menor o igual que el cometido por _TEAL_ a la salida de la red para cualquier nivel $K_i < n$ de activación deseado.\n",
    "\n",
    "- $ \\mathbb{E} \\left[ \\lVert y_{\\text{WINA}} - y \\rVert^2_2 \\right] \\le \\mathbb{E} \\left[ \\lVert y_{\\text{TEAL}} - y \\rVert^2_2 \\right] $\n",
    "\n",
    "En los apéndices del _paper_ puede encontrarse la demostración formal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663de5f2-a304-4017-bdd6-958953fbe708",
   "metadata": {},
   "source": [
    "### 2.7. Transformation Protocol\n",
    "\n",
    "La mayor limitación impuesta por la técnica es la condición de que todas las matrices de pesos deben tener columnas ortonormales, lo que no se puede garantizar en modo alguno.\n",
    "\n",
    "- $ W^T W = I_n $\n",
    "\n",
    "El _paper_ propone en este punto aplicar una transformación a las matrices de pesos.\n",
    "\n",
    "Utilizando _Singular Value Decomposition_ (_SVD_) descompone la matriz $W$ en tres matrices.\n",
    "\n",
    "- $ W = U \\Sigma V^T $\n",
    "\n",
    "Donde $U$ y $V$ son matrices con columnas ortonormales, y $\\Sigma$ es una matriz diagonal que contiene los valores singulares de $W$.\n",
    "\n",
    "Y realiza el producto de $W$ por $V$ que produce una nueva matriz $W'$ con columnas ortogonales, pero no necesariamente ortonormales.\n",
    "\n",
    "- $ W' = W V $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd0166-18ec-4e59-b603-d31023fb92e4",
   "metadata": {},
   "source": [
    "### 2.8. Computational Invariance\n",
    "\n",
    "Los cambios en las matrices de pesos cambian la salida del modelo, por lo que es necesario aplicar medidas para compensarlos.\n",
    "\n",
    "El _paper_ propone aplicar la técnica conocida como _computational invariance_, que establece que se puede aplicar una transformación ortogonal a la salida de un componente siempre que se deshaga en el siguiente.\n",
    "\n",
    "Por lo que, finalmente, para aplicar _WINA_ se debe forzar la condición impuesta a las matrices más relevantes de la red, como son las clásicas matrices de pesos $W_K$ del mecanismo de atención y $W_{gate}$ de _MLP_. Aplicar la técnica de _computacional invariance_ para propagar y ajustar el efecto  de los cambios realizados. Y utilizar la técnica basada en la norma L2 en estas matrices, y la basada únicamente en las magnitudes de las entradas en el resto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
