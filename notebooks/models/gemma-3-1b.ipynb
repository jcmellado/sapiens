{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02475974-8cb9-4950-8f0b-e1905bed6267",
   "metadata": {},
   "source": [
    "# Gemma 3 1B\n",
    "\n",
    "Revisión informal de la arquitectura del modelo Gemma 3 1B.\n",
    "\n",
    "Modelo de código abierto publicado por Google en marzo de 2025.\n",
    "\n",
    "- [https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)\n",
    "\n",
    "Mientras que la serie de modelos Gemma 3 es multimodal, el modelo 1B sólo soporta texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71a41a-d8ef-4117-bf45-c48628508f71",
   "metadata": {},
   "source": [
    "## 1. Arquitectura\n",
    "\n",
    "El modelo se basa en la arquitectura _Transformer_ que procesa una secuencia de entrada y genera una secuencia de salida.\n",
    "\n",
    "```\n",
    "Input ─> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b7903-bbee-4dc8-aabc-2a789d687227",
   "metadata": {},
   "source": [
    "### 1.1. Decoder-Only\n",
    "\n",
    "El modelo sólo implementa el _decoder_, prescidiendo del _encoder_ presente en la arquitectura clásica de los _transformers_.\n",
    "\n",
    "```\n",
    "Input ─> Decoder ─> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87038023-92ef-4876-ab2f-60816b4dd618",
   "metadata": {},
   "source": [
    "### 1.2. Decoder Stack\n",
    "\n",
    "El modelo utiliza un _stack_ de 26 _decoders_.\n",
    "\n",
    "```\n",
    "Input ─> 26 x Decoder ─> Output\n",
    "```\n",
    "\n",
    "La salida del primer _decoder_ alimenta al segundo _decoder_, la salida del segundo _decoder_ alimenta al tercer _decoder_, y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef5c9a-44bf-4839-848d-d1d98a25e7e4",
   "metadata": {},
   "source": [
    "## 2. Input\n",
    "\n",
    "El texto de entrada se procesa por un _tokenizer_, y a la entrada del modelo se aplica una capa de _embedding_.\n",
    "\n",
    "```\n",
    "Input ─> Embedding ─> 26 x Decoder ─> Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15da8d3-0282-43c4-8e32-e367d166c8e3",
   "metadata": {},
   "source": [
    "### 2.1. Tokenizer\n",
    "\n",
    "El modelo utiliza _SentencePiece_ como _tokenizer_.\n",
    "\n",
    "- [https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "La entrada del _tokenizer_ es texto libre de tamaño arbitrario, y su salida es una secuencia $T$ de _tokens_ de longitud $n$.\n",
    "\n",
    "Trata el texto como una secuencia de caracteres Unicode, de forma que no depende de espacios o delimitadores específicos de cada idioma para segmentarlo en _tokens_. Los _tokens_ se gestionan internamente como secuencias de _bytes_, técnica conocida como _byte-pair-encoding (BPE)_, por lo que puede procesar cualquier carácter o secuencia de caracteres aunque sean desconocidos.\n",
    "\n",
    "Los espacios en blanco se sustituyen por el carácter especial \"▁\" (_U+2581_), que se une al inicio de la palabra siguiente, y se tratan como _tokens_, de forma que se preserva la estructura del texto original. Por su parte, los dígitos numéricos se tratan como _tokens_ individuales, de forma que el número \"123\" se trata como tres _tokens_ individuales.\n",
    "\n",
    "El _tokenizer_ entrenado para el modelo se llama `Gemma3Tokenizer` y tiene un vocabulario de 256.128 entradas. Entre otros, tiene los clásicos _tokens_ de inicio (_BOS_) y final de secuencia (_EOS_), _padding_ (_PAD_), desconocido (_UNK_), inicio (_START_OF_TURN_) y final (_END_OF_TURN_) de turno de conversación cuando existen múltiples participantes, y los relativos a la codificación de imágenes que en la versión 1B no se utilizan.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ T \\in \\{0, 1, \\dots, V-1\\}^{1 \\times n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b66061-32e6-4edc-a472-c37327ee675b",
   "metadata": {},
   "source": [
    "### 2.2. Embedding\n",
    "\n",
    "La primera capa del modelo es una capa de _embedding_ de 1.152 dimensiones.\n",
    "\n",
    "Su matriz de pesos $W$ es inicializada con una distribución normal y actualizada durante el entrenamiento.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,1152}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{256128,1} & w_{256128,2} & \\ldots & w_{256128,1152}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La salida de esta capa se obtiene al indexar la matriz de pesos $W$ con cada _token_ de la secuencia de _tokens_ $T$.\n",
    "\n",
    "- $ \\text{EMB} = \\begin{bmatrix}\n",
    "W[T_1]\n",
    "\\\\ W[T_2]\n",
    "\\\\ \\vdots\n",
    "\\\\ W[T_n]\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Siguiendo el _paper_ original de la arquitectura _Transformer_, la salida se multiplica por la raíz cuadrada del número de dimensiones.\n",
    "\n",
    "- $ X = \\text{EMB} \\sqrt{1152} $\n",
    "\n",
    "La secuencia de _embeddings_ $X$ se propaga por la red sometiéndola a transformaciones funcionales a medida que avanza por las distintas capas de la red.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ T \\in \\{0, 1, \\dots, V-1\\}^{1 \\times n} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{256128 \\times 1152} $\n",
    "\n",
    "- $ \\text{EMB} \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "Huelga decir que el modelo funciona con _batches_, por lo que las dimensiones reales son (_tamaño de batch_, _longitud de secuencia_, _dimensiones de los embeddings_), pero por simplicidad en el análisis se omite.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = self.input_embedding_table[(x,)]\n",
    "x *= jnp.sqrt(self.embed_dim).astype(x.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a429405-b1c2-438b-95e5-ef51d35b7e96",
   "metadata": {},
   "source": [
    "## 3. Decoder\n",
    "\n",
    "Cada _decoder_ se compone de una capa de _atención_ y otra de _feed forward_ con capas intermedias de normalización y conexión residual.\n",
    "\n",
    "```\n",
    "Input ─> Embedding ─> 26x(─> Norm ─> Atn ─> Norm ─> Add ─> Norm ─> FF ─> Norm ─> Add) ─> Output\n",
    "                          │                          ↑  │                         ↑\n",
    "                          └──────────────────────────┘  └─────────────────────────┘\n",
    "```\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "inputs_normalized = self.pre_attention_norm(x)\n",
    "\n",
    "cache, attn_output = self.attn(\n",
    "    inputs_normalized,\n",
    "    segment_pos,\n",
    "    cache,\n",
    "    attn_mask,\n",
    ")\n",
    "\n",
    "attn_output = self.post_attention_norm(attn_output)\n",
    "\n",
    "attn_output += x\n",
    "\n",
    "outputs = self.pre_ffw_norm(attn_output)\n",
    "\n",
    "outputs = self.mlp(outputs)\n",
    "\n",
    "outputs = self.post_ffw_norm(outputs)\n",
    "\n",
    "outputs += attn_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7684f-47d1-4a2f-a658-f9b25ebd5abb",
   "metadata": {},
   "source": [
    "### 3.1. Norm (Pre-Attention)\n",
    "\n",
    "Esta capa normaliza la salida de la capa anterior con la función _RMSNorm_ (_Root Mean Square Layer Normalization_).\n",
    "\n",
    "- [https://arxiv.org/pdf/1910.07467](https://arxiv.org/pdf/1910.07467)\n",
    "\n",
    "La función ajusta los valores de entrada de acuerdo a la raíz cuadrada de la media de los cuadrados de los valores.\n",
    "\n",
    "- $ \\operatorname{RMS}(x) = \\sqrt{\\epsilon + \\cfrac{1}{n} \\sum\\limits_{i=1}^{n} x_i^2} $\n",
    "\n",
    "Los elementos de entrada se normalizan y multiplican por un factor de escala.\n",
    "\n",
    "- $ y_i = \\cfrac{x_i}{\\operatorname{RMS}(x)} (1 + \\gamma_i) $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y el vector de escalado $\\gamma$ de la capa se inicializa con ceros y se actualiza durante el entrenamiento.\n",
    "\n",
    "- $ \\gamma = \\begin{bmatrix} \\gamma_1 & \\gamma_2 & \\cdots & \\gamma_{1152} \\end{bmatrix} $\n",
    "\n",
    "Para el primer _decoder_, la salida de esta capa se calcula aplicando la función a su entrada, fila a fila.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(X_1)\n",
    "\\\\ \\operatorname{RMSNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(X_n)\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\cfrac{x_{1,1}}{\\operatorname{RMS}(X_1)} (1 + \\gamma_1) & \\cfrac{x_{1,2}}{\\operatorname{RMS}(X_1)} (1 + \\gamma_2) & \\ldots & \\cfrac{x_{1,1152}}{\\operatorname{RMS}(X_1)} (1 + \\gamma_{1152})\n",
    "\\\\ \\cfrac{x_{2,1}}{\\operatorname{RMS}(X_2)} (1 + \\gamma_1) & \\cfrac{x_{2,2}}{\\operatorname{RMS}(X_2)} (1 + \\gamma_2) & \\ldots & \\cfrac{x_{2,1152}}{\\operatorname{RMS}(X_2)} (1 + \\gamma_{1152})\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ \\cfrac{x_{n,1}}{\\operatorname{RMS}(X_n)} (1 + \\gamma_1) & \\cfrac{x_{n,2}}{\\operatorname{RMS}(X_n)} (1 + \\gamma_2) & \\ldots & \\cfrac{x_{n,1152}}{\\operatorname{RMS}(X_n)} (1 + \\gamma_{1152})\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Para el resto de _decoders_, la salida se calcula aplicando la función a la salida del _decoder_ anterior.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma \\in \\mathbb{R}^{1 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "scale = self.param('scale', nn.initializers.zeros_init(), (x.shape[-1]))\n",
    "\n",
    "var = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n",
    "scale = jnp.expand_dims(scale, axis=range(len(x.shape) - 1))\n",
    "\n",
    "normed_inputs = x * jax.lax.rsqrt(var + 1e-06)\n",
    "normed_inputs = normed_inputs * (1 + scale)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5bdfc-8ec0-41e7-a949-d6e9606f4e87",
   "metadata": {},
   "source": [
    "### 3.2. Attention (Atn)\n",
    "\n",
    "Esta capa aplica el mecanismo de atención.\n",
    "\n",
    "```\n",
    "Projection ─> QK-Norm ─> RoPE ─> Scale ─> Scores ─> Mask ─> SoftMax ─> Attention\n",
    "```\n",
    "\n",
    "Los modelos de la serie Gemma 3 implementan _Grouped Query Attention_ (_GQA_), pero el modelo 1B implementa _Multi-Query Attention_ (_MQA_).\n",
    "\n",
    "- [https://arxiv.org/pdf/1911.02150](https://arxiv.org/pdf/1911.02150)\n",
    "\n",
    "En la técnica más tradicional propuesta en el _paper_ original de la arquitectura _Transformer_ cada cabeza de atención tiene su propio conjunto de matrices $Q_i$, $K_i$ y $V_i$.\n",
    "\n",
    "En _MQA_ cada cabeza de atención sólo tiene una matriz $Q_i$, y existen dos matrices $K$ y $V$ compartidas.\n",
    "\n",
    "En _GQA_ cada cabeza de atención sólo tiene una matriz $Q_i$, y las cabezas se organizan en grupos, cada uno de ellos con dos matrices $K_j$ y $V_j$ compartidas por todas las cabezas del grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c64641-9f1e-44ee-9355-722fb64df725",
   "metadata": {},
   "source": [
    "#### 3.2.1. Projection\n",
    "\n",
    "La capa tiene cuatro matrices de pesos distintas para _Query_, una para _Key_, y otra para _Value_. Todas ellas inicializadas con una distribución normal y actualizadas durante el entrenamiento.\n",
    "\n",
    "- $ W_Q^1 \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "- $ W_Q^2 \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "- $ W_Q^3 \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "- $ W_Q^4 \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "- $ W_K \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "- $ W_V \\in \\mathbb{R}^{1152 \\times 256} $\n",
    "\n",
    "Cada matriz proyecta las dimensiones de los elementos de entrada de 1.152 a 256, manteniendo la longitud $n$ de la secuencia.\n",
    "\n",
    "- $ Q_1 = X W_Q^1 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^{q_1}_{1,1} & w^{q_1}_{1,2} & \\ldots & w^{q_1}_{1,256}\n",
    "\\\\ w^{q_1}_{2,1} & w^{q_1}_{2,2} & \\ldots & w^{q_1}_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^{q_1}_{1152,1} & w^{q_1}_{1152,2} & \\ldots & w^{q_1}_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "q^1_{1,1} & q^1_{1,2} & \\ldots & q^1_{1,256}\n",
    "\\\\ q^1_{2,1} & q^1_{2,2} & \\ldots & q^1_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^1_{n,1} & q^1_{n,2} & \\ldots & q^1_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ Q_2 = X W_Q^2 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^{q_2}_{1,1} & w^{q_2}_{1,2} & \\ldots & w^{q_2}_{1,256}\n",
    "\\\\ w^{q_2}_{2,1} & w^{q_2}_{2,2} & \\ldots & w^{q_2}_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^{q_2}_{1152,1} & w^{q_2}_{1152,2} & \\ldots & w^{q_2}_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "q^2_{1,1} & q^2_{1,2} & \\ldots & q^2_{1,256}\n",
    "\\\\ q^2_{2,1} & q^2_{2,2} & \\ldots & q^2_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^2_{n,1} & q^2_{n,2} & \\ldots & q^2_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ Q_3 = X W_Q^3 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^{q_3}_{1,1} & w^{q_3}_{1,2} & \\ldots & w^{q_3}_{1,256}\n",
    "\\\\ w^{q_3}_{2,1} & w^{q_3}_{2,2} & \\ldots & w^{q_3}_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^{q_3}_{1152,1} & w^{q_3}_{1152,2} & \\ldots & w^{q_3}_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "q^3_{1,1} & q^3_{1,2} & \\ldots & q^3_{1,256}\n",
    "\\\\ q^3_{2,1} & q^3_{2,2} & \\ldots & q^3_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^3_{n,1} & q^3_{n,2} & \\ldots & q^3_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ Q_4 = X W_Q^4 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^{q_4}_{1,1} & w^{q_4}_{1,2} & \\ldots & w^{q_4}_{1,256}\n",
    "\\\\ w^{q_4}_{2,1} & w^{q_4}_{2,2} & \\ldots & w^{q_4}_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^{q_4}_{1152,1} & w^{q_4}_{1152,2} & \\ldots & w^{q_4}_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "q^4_{1,1} & q^4_{1,2} & \\ldots & q^4_{1,256}\n",
    "\\\\ q^4_{2,1} & q^4_{2,2} & \\ldots & q^4_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^4_{n,1} & q^4_{n,2} & \\ldots & q^4_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ K = X W_K = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^k_{1,1} & w^k_{1,2} & \\ldots & w^k_{1,256}\n",
    "\\\\ w^k_{2,1} & w^k_{2,2} & \\ldots & w^k_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^k_{1152,1} & w^k_{1152,2} & \\ldots & w^k_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & \\ldots & k_{1,256}\n",
    "\\\\ k_{2,1} & k_{2,2} & \\ldots & k_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ k_{n,1} & k_{n,2} & \\ldots & k_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ V = X W_V = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^v_{1,1} & w^v_{1,2} & \\ldots & w^v_{1,256}\n",
    "\\\\ w^v_{2,1} & w^v_{2,2} & \\ldots & w^v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^v_{1152,1} & w^v_{1152,2} & \\ldots & w^v_{1152,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "v_{1,1} & v_{1,2} & \\ldots & v_{1,256}\n",
    "\\\\ v_{2,1} & v_{2,2} & \\ldots & v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ v_{n,1} & v_{n,2} & \\ldots & v_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ Q_1 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_2 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_3 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_4 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ K \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ V \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "Para evitar repetir cálculos, y reducir la memoria utilizada, se utiliza una _cache_ sobre las matrices $K$ y $V$ de la capa de atención de cada _decoder_.\n",
    "\n",
    "Huelga decir que el modelo gestiona eficientemente los cálculos con matrices, pero por simplicidad en el análisis se muestra una capa con sus matrices claramente diferenciadas sobre las que se aplican operaciones de forma individual paso a paso.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.q_einsum = _layers.Einsum(\n",
    "  shape=(self.num_heads, self.features, self.head_dim),\n",
    ")\n",
    "self.kv_einsum = _layers.Einsum(\n",
    "  shape=(2, self.num_kv_heads, self.features, self.head_dim),\n",
    ")\n",
    "\n",
    "query_proj = self.q_einsum('BTD,NDH->BTNH', x)\n",
    "key_proj, value_proj = self.kv_einsum('BSD,CKDH->CBSKH', x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228904b5-ea6f-4d04-9e6c-dcc609ce7fa3",
   "metadata": {},
   "source": [
    "#### 3.2.2. QK-Norm\n",
    "\n",
    "Las proyecciones $Q_i$ y $K$ se normalizan aplicando _RMSNorm_ fila a fila.\n",
    "\n",
    "- $ Q_1 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(Q^1_1)\n",
    "\\\\ \\operatorname{RMSNorm}(Q^1_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(Q^1_n)\n",
    "\\end{bmatrix} \\quad Q_2 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(Q^2_1)\n",
    "\\\\ \\operatorname{RMSNorm}(Q^2_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(Q^2_n)\n",
    "\\end{bmatrix} \\quad Q_3 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(Q^3_1)\n",
    "\\\\ \\operatorname{RMSNorm}(Q^3_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(Q^3_n)\n",
    "\\end{bmatrix} \\quad Q_4 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(Q^4_1)\n",
    "\\\\ \\operatorname{RMSNorm}(Q^4_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(Q^4_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ K \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(K_1)\n",
    "\\\\ \\operatorname{RMSNorm}(K_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(K_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y los vectores de escalado $\\gamma^Q$ y $\\gamma^K$ de la capa se inicializan con ceros y se actualizan durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma^Q \\in \\mathbb{R}^{1 \\times 256} $\n",
    "\n",
    "- $ \\gamma^K \\in \\mathbb{R}^{1 \\times 256} $\n",
    "\n",
    "Las dimensiones de las matrices de proyección no se ven afectadas.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "query_proj = self._query_norm(query_proj)\n",
    "key_proj = self._key_norm(key_proj)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61fbe1c-2d58-4f58-9ada-dc58382baba9",
   "metadata": {},
   "source": [
    "#### 3.2.3. 5-to-1 Interleaved Attention\n",
    "\n",
    "El modelo intercala capas de atención local con capas de atención global.\n",
    "\n",
    "Cada 5 capas de atención local intercala 1 capa de atención global.\n",
    "\n",
    "El modelo tiene una ventana de contexto de 32.768 (32K) _tokens_.\n",
    "\n",
    "Las capas de atención local tienen una ventana de contexto de 512 _tokens_.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "GEMMA3_ATTENTION_PATTERN = (\n",
    "    _modules.AttentionType.LOCAL_SLIDING,\n",
    "    _modules.AttentionType.LOCAL_SLIDING,\n",
    "    _modules.AttentionType.LOCAL_SLIDING,\n",
    "    _modules.AttentionType.LOCAL_SLIDING,\n",
    "    _modules.AttentionType.LOCAL_SLIDING,\n",
    "    _modules.AttentionType.GLOBAL,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acb6bd-0f18-4df6-a591-cad2636fde85",
   "metadata": {},
   "source": [
    "#### 3.2.4. RoPE\n",
    "\n",
    "Las proyecciones $Q_i$ y $K$ se enriquecen con información posicional mediante _Rotary Positional Embedding_ (_RoPE_).\n",
    "\n",
    "- [https://arxiv.org/pdf/2104.09864](https://arxiv.org/pdf/2104.09864)\n",
    "\n",
    "Esta técnica es la más usada actualmente, en contraposición a la técnica _Positional Encoding_ propuesta en el _paper_ original de la arquitectura _Transformer_.\n",
    "\n",
    "- $ Q_1 \\leftarrow \\operatorname{RoPE}(Q_1)\n",
    "\\quad Q_2 \\leftarrow \\operatorname{RoPE}(Q_2)\n",
    "\\quad Q_3 \\leftarrow \\operatorname{RoPE}(Q_3)\n",
    "\\quad Q_4 \\leftarrow \\operatorname{RoPE}(Q_4) $\n",
    "\n",
    "- $ K \\leftarrow \\operatorname{RoPE}(K) $\n",
    "\n",
    "Se aplica una función $RoPE(m, j)$ a las matrices $Q_i$ y $K$ que se evalúa para cada par $(m, j)$. Donde $m$ es una posición dentro de la secuencia y $j$ es una pareja de elementos de las matrices.\n",
    "\n",
    "Es decir, se recorren todas las filas para $m = 1, 2, ..., n$, y se toman los elementos de cada fila de dos en dos $(x_{2j}, x_{2j+1})$ para $j = 0, 1, \\ldots, \\cfrac{256}{2} - 1$ (por convención se empieza en $0$).\n",
    "\n",
    "Para cada par $(m, j)$ se calcula un ángulo $\\theta_{m,j}$, y se rota cada pareja de elementos por dicho ángulo como si fueran coordenadas 2D.\n",
    "\n",
    "- $ \\theta_{m,j} = \\cfrac{m}{\\text{baseFrequency}^{\\left(\\cfrac{2j}{256}\\right)}} $\n",
    "\n",
    "- $ x'_{2j} = x_{2j} \\cos(\\theta_{m,j}) - x_{2j+1} \\sin(\\theta_{m,j}) $\n",
    "\n",
    "- $ x'_{2j+1} = x_{2j} \\sin(\\theta_{m,j}) + x_{2j+1} \\cos(\\theta_{m,j}) $\n",
    "\n",
    "Las capas de atención local utilizan un frecuencia base de 10K, y las de atención global de 1M.\n",
    "\n",
    "Las dimensiones de las matrices de proyección no se ven afectadas.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "query_proj = _positional_embeddings.apply_rope(\n",
    "  query_proj,\n",
    "  segment_pos,\n",
    "  base_frequency=self.rope_base_frequency,\n",
    "  scale_factor=self.rope_scale_factor,\n",
    ")\n",
    "\n",
    "key_proj = _positional_embeddings.apply_rope(\n",
    "  key_proj,\n",
    "  segment_pos,\n",
    "  base_frequency=self.rope_base_frequency,\n",
    "  scale_factor=self.rope_scale_factor,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3727318-0f85-4afe-848d-66fb983415b9",
   "metadata": {},
   "source": [
    "#### 3.2.5. Scale\n",
    "\n",
    "Las proyecciones $Q_i$ se escalan por la inversa de la raíz cuadrada de las dimensiones de las cabezas de atención siguiendo el _paper_ original de la arquitectura _Transformer_.\n",
    "\n",
    "- $ Q_1 \\leftarrow Q_1 \\cfrac{1}{\\sqrt{256}}\n",
    "\\quad Q_2 \\leftarrow Q_2 \\cfrac{1}{\\sqrt{256}}\n",
    "\\quad Q_3 \\leftarrow Q_3 \\cfrac{1}{\\sqrt{256}}\n",
    "\\quad Q_4 \\leftarrow Q_4 \\cfrac{1}{\\sqrt{256}} $\n",
    "\n",
    "Las dimensiones de las matrices de proyección no se ven afectadas.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "query_scaled = query_proj * self.query_pre_attn_scalar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7cdb5-e904-4427-96fe-04d560e78e93",
   "metadata": {},
   "source": [
    "#### 3.2.6. Scores\n",
    "\n",
    "La matrices de \"_scores_\" se calculan multiplicando las proyecciones $Q_i$ por la traspuesta de $K$.\n",
    "\n",
    "- $ S_1 = Q_1 K^T = \\begin{bmatrix}\n",
    "q^1_{1,1} & q^1_{1,2} & \\ldots & q^1_{1,256}\n",
    "\\\\ q^1_{2,1} & q^1_{2,2} & \\ldots & q^1_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^1_{n,1} & q^1_{n,2} & \\ldots & q^1_{n,256}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & \\ldots & k_{1,n}\n",
    "\\\\ k_{2,1} & k_{2,2} & \\ldots & k_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ k_{256,1} & k_{256,2} & \\ldots & k_{256,n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "s^1_{1,1} & s^1_{1,2} & \\ldots & s^1_{1,n}\n",
    "\\\\ s^1_{2,1} & s^1_{2,2} & \\ldots & s^1_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^1_{n,1} & s^1_{n,2} & \\ldots & s^1_{n,n}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ S_2 = Q_2 K^T = \\begin{bmatrix}\n",
    "q^2_{1,1} & q^2_{1,2} & \\ldots & q^2_{1,256}\n",
    "\\\\ q^2_{2,1} & q^2_{2,2} & \\ldots & q^2_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^2_{n,1} & q^2_{n,2} & \\ldots & q^2_{n,256}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & \\ldots & k_{1,n}\n",
    "\\\\ k_{2,1} & k_{2,2} & \\ldots & k_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ k_{256,1} & k_{256,2} & \\ldots & k_{256,n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "s^2_{1,1} & s^2_{1,2} & \\ldots & s^2_{1,n}\n",
    "\\\\ s^2_{2,1} & s^2_{2,2} & \\ldots & s^2_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^2_{n,1} & s^2_{n,2} & \\ldots & s^2_{n,n}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ S_3 = Q_3 K^T = \\begin{bmatrix}\n",
    "q^3_{1,1} & q^3_{1,2} & \\ldots & q^3_{1,256}\n",
    "\\\\ q^3_{2,1} & q^3_{2,2} & \\ldots & q^3_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^3_{n,1} & q^3_{n,2} & \\ldots & q^3_{n,256}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & \\ldots & k_{1,n}\n",
    "\\\\ k_{2,1} & k_{2,2} & \\ldots & k_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ k_{256,1} & k_{256,2} & \\ldots & k_{256,n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "s^3_{1,1} & s^3_{1,2} & \\ldots & s^3_{1,n}\n",
    "\\\\ s^3_{2,1} & s^3_{2,2} & \\ldots & s^3_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^3_{n,1} & s^3_{n,2} & \\ldots & s^3_{n,n}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ S_4 = Q_4 K^T = \\begin{bmatrix}\n",
    "q^4_{1,1} & q^4_{1,2} & \\ldots & q^4_{1,256}\n",
    "\\\\ q^4_{2,1} & q^4_{2,2} & \\ldots & q^4_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ q^4_{n,1} & q^4_{n,2} & \\ldots & q^4_{n,256}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "k_{1,1} & k_{1,2} & \\ldots & k_{1,n}\n",
    "\\\\ k_{2,1} & k_{2,2} & \\ldots & k_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ k_{256,1} & k_{256,2} & \\ldots & k_{256,n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "s^4_{1,1} & s^4_{1,2} & \\ldots & s^4_{1,n}\n",
    "\\\\ s^4_{2,1} & s^4_{2,2} & \\ldots & s^4_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^4_{n,1} & s^4_{n,2} & \\ldots & s^4_{n,n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Esta operación crea matrices cuadradas de $n \\times n$, donde cada elemento representa la atención que cada _token_ presta a si mismo y al resto de _tokens_ de la secuencia.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ Q_1 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_2 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_3 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ Q_4 \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ K \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ S_1 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_2 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_3 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_4 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "logits = jnp.einsum('BTNH,BSNH->BTNS', query_scaled, key_proj)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3d9d5-e7b8-4f7b-8558-37ccaa57da5e",
   "metadata": {},
   "source": [
    "#### 3.2.7. Mask\n",
    "\n",
    "Aunque se ha omitido por simplicidad en el análisis, el modelo trabaja con _batches_. Es decir, procesa varias secuencias de entrada a un mismo tiempo.\n",
    "\n",
    "Como cada secuencia puede tener una longitud distinta, se utilizan _tokens_ de _padding_ para completar la secuencias más cortas, y que todas tengan la misma longitud.\n",
    "\n",
    "Los _tokens_ de _padding_ se ignoran durante el procesamiento mediante máscaras. Siendo las máscaras matrices numéricas o lógicas que indican que elementos se deben ignorar.\n",
    "\n",
    "De igual forma, un _token_ $i$ no puede prestar atención a un _token_ $j$ que aún no se ha generado. Por lo que se aplican máscaras sobre las matrices de _scores_.\n",
    "\n",
    "Además, para las capas de atención local se construyen máscaras de atención local sobre el tamaño de la ventana de contexto local, que en este modelo es de 512 _tokens_.\n",
    "\n",
    "Las máscaras se combinan y aplican sobre la matriz de _scores_, sustituyendo los valores a ignorar por un número negativo muy pequeño. En el _paper_ original de la arquitectura _Transformers_ se propone utilizar $-\\infty$, pero el modelo utiliza una constante con valor $-2.3819763e38$.\n",
    "\n",
    "- $ S_1 \\leftarrow \\operatorname{mask}(S_1)\n",
    "\\quad S_2 \\leftarrow \\operatorname{mask}(S_2)\n",
    "\\quad S_3 \\leftarrow \\operatorname{mask}(S_3)\n",
    "\\quad S_4 \\leftarrow \\operatorname{mask}(S_4) $\n",
    "\n",
    "Las dimensiones de las matrices de _scores_ no se ven afectadas.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "if self.attn_type == AttentionType.LOCAL_SLIDING:\n",
    "  sliding_mask = _create_sliding_mask(\n",
    "    segment_pos,\n",
    "    end_index=cache['end_index'][0] if cache is not None else 0,\n",
    "    cache_len=attn_mask.shape[-1],\n",
    "    sliding_window_size=self.sliding_window_size,\n",
    "  )\n",
    "  attn_mask *= sliding_mask\n",
    "\n",
    "padded_logits = jnp.where((jnp.expand_dims(attn_mask, -2)), logits, K_MASK)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad566afa-1164-4942-b78a-e6bf10e3e0ff",
   "metadata": {},
   "source": [
    "#### 3.2.8. SoftMax\n",
    "\n",
    "Los valores de las matrices de _scores_ se convierten a una distribución de probabilidad aplicando la función $softmax$ fila a fila.\n",
    "\n",
    "- $ S_1 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{softmax}(S^1_1)\n",
    "\\\\ \\operatorname{softmax}(S^1_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{softmax}(S^1_n)\n",
    "\\end{bmatrix}\n",
    "\\quad S_2 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{softmax}(S^2_1)\n",
    "\\\\ \\operatorname{softmax}(S^2_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{softmax}(S^2_n)\n",
    "\\end{bmatrix}\n",
    "\\quad S_3 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{softmax}(S^3_1)\n",
    "\\\\ \\operatorname{softmax}(S^3_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{softmax}(S^3_n)\n",
    "\\end{bmatrix}\n",
    "\\quad S_4 \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{softmax}(S^4_1)\n",
    "\\\\ \\operatorname{softmax}(S^4_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{softmax}(S^4_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La función $softmax$ comprime valores que se encuentra dentro de un rango arbitrario al rango $[0, 1]$, de forma que la suma de cada fila sea igual a $1$.\n",
    "\n",
    "- $ \\operatorname{softmax}(x)_i = \\cfrac{e^{x_i}}{\\sum\\limits_{j=1}^{d} e^{x_j}} $\n",
    "\n",
    "Debe ser claro que al aplicar la máscara de atención y sustituir los valores de los elementos de la matriz no deseados por $-\\infty$ (o un número negativo muy pequeño) tiene el efecto de que se ignoren, ya que $e^{-\\infty} = 0$.\n",
    "\n",
    "Las dimensiones de las matrices de _scores_ no se ven afectadas.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "probs = jax.nn.softmax(padded_logits, axis=-1).astype(key_proj.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166fccc-b0c9-49fd-acc0-da5642762cbf",
   "metadata": {},
   "source": [
    "#### 3.2.9. Attention\n",
    "\n",
    "Las matrices de _scores_ se proyectan por la matriz de _Value_ para obtener las matrices de atención.\n",
    "\n",
    "- $ A_1 = S_1 V = \\begin{bmatrix}\n",
    "s^1_{1,1} & s^1_{1,2} & \\ldots & s^1_{1,n}\n",
    "\\\\ s^1_{2,1} & s^1_{2,2} & \\ldots & s^1_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^1_{n,1} & s^1_{n,2} & \\ldots & s^1_{n,n}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "v_{1,1} & v_{1,2} & \\ldots & v_{1,256}\n",
    "\\\\ v_{2,1} & v_{2,2} & \\ldots & v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ v_{n,1} & v_{n,2} & \\ldots & v_{n,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a^1_{1,1} & a^1_{1,2} & \\ldots & a^1_{1,256}\n",
    "\\\\ a^1_{2,1} & a^1_{2,2} & \\ldots & a^1_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a^1_{n,1} & a^1_{n,2} & \\ldots & a^1_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ A_2 = S_2 V = \\begin{bmatrix}\n",
    "s^2_{1,1} & s^2_{1,2} & \\ldots & s^2_{1,n}\n",
    "\\\\ s^2_{2,1} & s^2_{2,2} & \\ldots & s^2_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^2_{n,1} & s^2_{n,2} & \\ldots & s^2_{n,n}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "v_{1,1} & v_{1,2} & \\ldots & v_{1,256}\n",
    "\\\\ v_{2,1} & v_{2,2} & \\ldots & v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ v_{n,1} & v_{n,2} & \\ldots & v_{n,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a^2_{1,1} & a^2_{1,2} & \\ldots & a^2_{1,256}\n",
    "\\\\ a^2_{2,1} & a^2_{2,2} & \\ldots & a^2_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a^2_{n,1} & a^2_{n,2} & \\ldots & a^2_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ A_3 = S_3 V = \\begin{bmatrix}\n",
    "s^3_{1,1} & s^3_{1,2} & \\ldots & s^3_{1,n}\n",
    "\\\\ s^3_{2,1} & s^3_{2,2} & \\ldots & s^3_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^3_{n,1} & s^3_{n,2} & \\ldots & s^3_{n,n}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "v_{1,1} & v_{1,2} & \\ldots & v_{1,256}\n",
    "\\\\ v_{2,1} & v_{2,2} & \\ldots & v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ v_{n,1} & v_{n,2} & \\ldots & v_{n,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a^3_{1,1} & a^3_{1,2} & \\ldots & a^3_{1,256}\n",
    "\\\\ a^3_{2,1} & a^3_{2,2} & \\ldots & a^3_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a^3_{n,1} & a^3_{n,2} & \\ldots & a^3_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ A_4 = S_4 V = \\begin{bmatrix}\n",
    "s^4_{1,1} & s^4_{1,2} & \\ldots & s^4_{1,n}\n",
    "\\\\ s^4_{2,1} & s^4_{2,2} & \\ldots & s^4_{2,n}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ s^4_{n,1} & s^4_{n,2} & \\ldots & s^4_{n,n}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "v_{1,1} & v_{1,2} & \\ldots & v_{1,256}\n",
    "\\\\ v_{2,1} & v_{2,2} & \\ldots & v_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ v_{n,1} & v_{n,2} & \\ldots & v_{n,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a^4_{1,1} & a^4_{1,2} & \\ldots & a^4_{1,256}\n",
    "\\\\ a^4_{2,1} & a^4_{2,2} & \\ldots & a^4_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a^4_{n,1} & a^4_{n,2} & \\ldots & a^4_{n,256}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Las matrices de atención se concatenan para formar una única matriz.\n",
    "\n",
    "- $ A = \\operatorname{concat}(A_1, A_2, A_3, A_4) $\n",
    "\n",
    "- $ A = \\begin{bmatrix}\n",
    "a^1_{1,1} & a^1_{1,2} & \\ldots & a^1_{1,256} & a^2_{1,1} & a^2_{1,2} & \\ldots & a^2_{1,256} & a^3_{1,1} & a^3_{1,2} & \\ldots & a^3_{1,256} & a^4_{1,1} & a^4_{1,2} & \\ldots & a^4_{1,256}\n",
    "\\\\ a^1_{2,1} & a^1_{2,2} & \\ldots & a^1_{2,256} & a^2_{2,1} & a^2_{2,2} & \\ldots & a^2_{2,256} & a^3_{2,1} & a^3_{2,2} & \\ldots & a^3_{2,256} & a^4_{2,1} & a^4_{2,2} & \\ldots & a^4_{2,256}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a^1_{n,1} & a^1_{n,2} & \\ldots & a^1_{n,256} & a^2_{n,1} & a^2_{n,2} & \\ldots & a^2_{n,256} & a^3_{n,1} & a^3_{n,2} & \\ldots & a^3_{n,256} & a^4_{n,1} & a^4_{n,2} & \\ldots & a^4_{n,256}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & \\ldots & a_{1,1024}\n",
    "\\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,1024}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,1024}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Y finalmente, la salida de capa de atención se calcula proyectando $A$ por una nueva matriz $W_O$, inicializada con una distribución normal y actualizada durante el entrenamiento, que retorna la atención a las dimensiones del modelo.\n",
    "\n",
    "- $ X \\leftarrow A W_O = \\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & \\ldots & a_{1,1024}\n",
    "\\\\ a_{2,1} & a_{2,2} & \\ldots & a_{2,1024}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ a_{n,1} & a_{n,2} & \\ldots & a_{n,1024}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^o_{1,1} & w^o_{1,2} & \\ldots & w^o_{1,1152}\n",
    "\\\\ w^o_{2,1} & w^o_{2,2} & \\ldots & w^o_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^o_{1024,1} & w^o_{1024,2} & \\ldots & w^o_{1024,1152}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ S_1 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_2 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_3 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ S_4 \\in \\mathbb{R}^{n \\times n} $\n",
    "\n",
    "- $ V \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ A_i \\in \\mathbb{R}^{n \\times 256} $\n",
    "\n",
    "- $ A \\in \\mathbb{R}^{n \\times 1024} $\n",
    "\n",
    "- $ W_o \\in \\mathbb{R}^{1024 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "encoded = jnp.einsum('BTNS,BSNH->BTNH', probs, value_proj)\n",
    "\n",
    "attn_output = self.attn_vec_einsum('BTNH,NHD->BTD', encoded)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0777ef-e61c-4308-90ec-c2face24bc41",
   "metadata": {},
   "source": [
    "### 3.3. Norm (Post-Attention)\n",
    "\n",
    "Esta capa normaliza la salida de la capa anterior con _RMSNorm_.\n",
    "\n",
    "La salida de esta capa se calcula aplicando la función a su entrada fila a fila.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(X_1)\n",
    "\\\\ \\operatorname{RMSNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y el vector de escalado $\\gamma$ de la capa se inicializa con ceros y se actualiza durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma \\in \\mathbb{R}^{1 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49102a6c-6448-42f6-8206-6ea0c08f9006",
   "metadata": {},
   "source": [
    "### 3.4. Add (Post-Attention)\n",
    "\n",
    "Esta capa realiza una conexión residual para propagar información presente antes de la atención.\n",
    "\n",
    "Para el primer _decoder_, la salida de esta capa se calcula sumando a la salida de la capa anterior la salida de la capa inicial de _embeddings_.\n",
    "\n",
    "- $ \\text{ATN} = X + \\text{EMB} $\n",
    "\n",
    "Y para el resto de _decoders_, la salida se calcula sumando a la salida de la capa anterior la salida del _decoder_ anterior.\n",
    "\n",
    "La salida de este paso es el resultado de la suma.\n",
    "\n",
    "- $ X \\leftarrow \\text{ATN} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "- $ \\text{EMB} \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "- $ \\text{ATN} \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29115a-14bb-4029-bd0d-d3714d3f4bcb",
   "metadata": {},
   "source": [
    "### 3.5. Norm (Pre-Feed Forward)\n",
    "\n",
    "Esta capa normaliza la salida de la capa anterior con _RMSNorm_.\n",
    "\n",
    "La salida de esta capa se calcula aplicando la función a su entrada fila a fila.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(X_1)\n",
    "\\\\ \\operatorname{RMSNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y el vector de escalado $\\gamma$ de la capa se inicializa con ceros y se actualiza durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma \\in \\mathbb{R}^{1 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f2d40-db1a-48a5-ab6f-569a20faf067",
   "metadata": {},
   "source": [
    "### 3.6. Feed Forward (FF)\n",
    "\n",
    "Esta capa añade no linealidad expandiendo las dimensiones del modelo para mejorar la potencia expresiva del mismo.\n",
    "\n",
    "Utiliza _GeGLU_, una variante de _GLU_ (_Gated Linear Unit_) con la función de activación _GELU_.\n",
    "\n",
    "- [https://arxiv.org/pdf/2002.05202](https://arxiv.org/pdf/2002.05202)\n",
    "\n",
    "Aplica dos proyecciones lineales a partir de los datos de entrada para expandir las dimensiones originales de los _embeddings_, de 1.152 a 6.912 (seis veces el tamaño original).\n",
    "\n",
    "- $ g_1 = x W_1 $\n",
    "\n",
    "- $ g_2 = x W_2 $\n",
    "\n",
    "Aplica la función _GELU_ a la primera proyección y multiplica el resultado, elemento a elemento, por la segunda proyección.\n",
    "\n",
    "- $ h = \\operatorname{GELU}(g_1) \\otimes g_2 $\n",
    "\n",
    "Y, finalmente, reduce las dimensiones a las originales, de 6.912 a 1.152.\n",
    "\n",
    "- $ y = h W_3 $\n",
    "\n",
    "La segunda proyección funciona como una puerta de control (_gate_), que deja pasar una cantidad mayor o menor de información del resultado de aplicar la función de activación a la primera proyección.\n",
    "\n",
    "En total hay tres matrices de pesos que son inicializadas con una distribución normal y actualizadas durante el entrenamiento.\n",
    "\n",
    "La salida de esta capa se calcula aplicando _GeGLU_ y las proyecciones a su entrada.\n",
    "\n",
    "- $ g_1 = X W_1 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^1_{1,1} & w^1_{1,2} & \\ldots & w^1_{1,6912}\n",
    "\\\\ w^1_{2,1} & w^1_{2,2} & \\ldots & w^1_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^1_{1152,1} & w^1_{1152,2} & \\ldots & w^1_{1152,6912}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "g^1_{1,1} & g^1_{1,2} & \\ldots & g^1_{1,6912}\n",
    "\\\\ g^1_{2,1} & g^1_{2,2} & \\ldots & g^1_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ g^1_{n,1} & g^1_{n,2} & \\ldots & g^1_{n,6912}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ g_2 = X W_2 = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^2_{1,1} & w^2_{1,2} & \\ldots & w^2_{1,6912}\n",
    "\\\\ w^2_{2,1} & w^2_{2,2} & \\ldots & w^2_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^2_{1152,1} & w^2_{1152,2} & \\ldots & w^2_{1152,6912}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "g^2_{1,1} & g^2_{1,2} & \\ldots & g^2_{1,6912}\n",
    "\\\\ g^2_{2,1} & g^2_{2,2} & \\ldots & g^2_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ g^2_{n,1} & g^2_{n,2} & \\ldots & g^2_{n,6912}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ h = \\operatorname{GELU}(g_1) \\otimes g_2 = \\begin{bmatrix}\n",
    "\\operatorname{GELU}(g^1_{1,1}) g^2_{1,1} & \\operatorname{GELU}(g^1_{1,2}) g^2_{1,2} & \\ldots & \\operatorname{GELU}(g^1_{1,6912}) g^2_{1,6912}\n",
    "\\\\ \\operatorname{GELU}(g^1_{2,1}) g^2_{2,1} & \\operatorname{GELU}(g^1_{2,2}) g^2_{2,2} & \\ldots & \\operatorname{GELU}(g^1_{2,6912}) g^2_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ \\operatorname{GELU}(g^1_{n,1}) g^2_{n,1} & \\operatorname{GELU}(g^1_{n,2}) g^2_{n,2} & \\ldots & \\operatorname{GELU}(g^1_{n,6912}) g^2_{n,6912}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h_{1,1} & h_{1,2} & \\ldots & h_{1,6912}\n",
    "\\\\ h_{2,1} & h_{2,2} & \\ldots & h_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ h_{n,1} & h_{n,2} & \\ldots & h_{n,6912}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "- $ X \\leftarrow h W_3 = \\begin{bmatrix}\n",
    "h_{1,1} & h_{1,2} & \\ldots & h_{1,6912}\n",
    "\\\\ h_{2,1} & h_{2,2} & \\ldots & h_{2,6912}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ h_{n,1} & h_{n,2} & \\ldots & h_{n,6912}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w^3_{1,1} & w^3_{1,2} & \\ldots & w^3_{1,1152}\n",
    "\\\\ w^3_{2,1} & w^3_{2,2} & \\ldots & w^3_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w^3_{6912,1} & w^3_{6912,2} & \\ldots & w^3_{6912,1152}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ W_1 \\in \\mathbb{R}^{1152 \\times 6912} $\n",
    "\n",
    "- $ g_1 \\in \\mathbb{R}^{n \\times 6912} $\n",
    "\n",
    "- $ W_2 \\in \\mathbb{R}^{1152 \\times 6912} $\n",
    "\n",
    "- $ g_2 \\in \\mathbb{R}^{n \\times 6912} $\n",
    "\n",
    "- $ h \\in \\mathbb{R}^{n \\times 6912} $\n",
    "\n",
    "- $ W_3 \\in \\mathbb{R}^{6912 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "gating = _layers.Einsum(\n",
    "  shape=(2, self.features, self.hidden_dim),\n",
    "  weight_name='gating_einsum',\n",
    ")\n",
    "\n",
    "gate = gating(eq, x)\n",
    "\n",
    "activations = nn.gelu(gate[..., 0, :]) * gate[..., 1, :]\n",
    "\n",
    "linear = _layers.Einsum(\n",
    "  shape=(self.hidden_dim, self.features),\n",
    "  weight_name='linear',\n",
    ")\n",
    "\n",
    "outputs = linear('...H,HF->...F', activations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb783a-138a-44bc-ad39-c635bc5cb0e9",
   "metadata": {},
   "source": [
    "### 3.7. Norm (Post-Feed Forward)\n",
    "\n",
    "Esta capa normaliza la salida de la capa anterior con _RMSNorm_.\n",
    "\n",
    "La salida de esta capa se calcula aplicando la función a su entrada fila a fila.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(X_1)\n",
    "\\\\ \\operatorname{RMSNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y el vector de escalado $\\gamma$ de la capa se inicializa con ceros y se actualiza durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma \\in \\mathbb{R}^{1 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26c83e-e270-4b17-b7b1-8279dcd8f9f6",
   "metadata": {},
   "source": [
    "### 3.8. Add (Post-Feed Forward)\n",
    "\n",
    "Esta capa realiza una conexión residual para propagar información presente antes de añadir la no linealidad.\n",
    "\n",
    "La salida de esta capa se calcula sumando a la salida de la capa anterior la salida de la atención.\n",
    "\n",
    "- $ X \\leftarrow X + \\text{ATN} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "- $ \\text{ATN} \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6506a-695b-41b2-894f-42d947ff9800",
   "metadata": {},
   "source": [
    "## 4. Output\n",
    "\n",
    "En la salida del modelo se aplica una normalización final y se obtienen las probabilidades (no normalizadas) para cada _token_ del vocabulario.\n",
    "\n",
    "```\n",
    "Input ─> Embedding ─> 26 x Decoder ─> Norm ─> Logits -> Output\n",
    "```\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = self.final_norm(x)\n",
    "\n",
    "logits = self.embedder.decode(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99ad94-59d1-4572-a719-9999148f9bf5",
   "metadata": {},
   "source": [
    "### 4.1. Norm (Final)\n",
    "\n",
    "Esta capa normaliza la salida de la capa anterior con _RMSNorm_.\n",
    "\n",
    "La salida de esta capa se calcula aplicando la función a la salida del último _decoder_ fila a fila.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{RMSNorm}(X_1)\n",
    "\\\\ \\operatorname{RMSNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{RMSNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La constante $\\epsilon$ tiene valor $10^{-6}$. Y el vector de escalado $\\gamma$ de la capa se inicializa con ceros y se actualiza durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ \\gamma \\in \\mathbb{R}^{1 \\times 1152} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c841d37-1152-4d59-91be-57af1d46c6de",
   "metadata": {},
   "source": [
    "### 4.2. Logits\n",
    "\n",
    "Finalmente, la salida del modelo son los conocidos como \"_logits_\".\n",
    "\n",
    "Es decir, las puntuaciones brutas (_scores_) para cada _token_ del vocabulario que indican la probabilidad (no normalizada) que tienen de ser el siguiente de la secuencia de salida.\n",
    "\n",
    "La capa proyecta las dimensiones internas del modelo a las dimensiones del vocabulario, de 1.152 a 256.128, a través de la traspuesta de la matriz de pesos de la capa de _embedding_ utilizada a la entrada del modelo.\n",
    "\n",
    "La salida de esta capa se calcula multiplicando su entrada por la traspuesta de la matriz de pesos.\n",
    "\n",
    "- $ Y = X W^T = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,1152}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,1152}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,1152}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,256128}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,256128}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{1152,1} & w_{1152,2} & \\ldots & w_{1152,256128}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "y_{1,1} & y_{1,2} & \\ldots & y_{1,256128}\n",
    "\\\\ y_{2,1} & y_{2,2} & \\ldots & y_{2,256128}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ y_{n,1} & y_{n,2} & \\ldots & y_{n,256128}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times 1152} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{256128 \\times 1152} $\n",
    "\n",
    "- $ Y \\in \\mathbb{R}^{n \\times 256128} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "return jnp.dot(x, self.input_embedding_table.T)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ff421-098a-49d7-a983-5d3aee55d32a",
   "metadata": {},
   "source": [
    "### 4.3. Sampling\n",
    "\n",
    "Para convertir la matriz de _logits_ en _tokens_ se utiliza una estrategia de _sampling_. Por ejemplo, una estrategia _greedy_ buscará simplemente el _token_ del vocabulario con el _logit_ (puntuación) más alta. Otras estrategias pueden utilizar criterios más complejos y añadir parámetros como la temperatura para seleccionar los _tokens_.\n",
    "\n",
    "De igual forma que el _tokenizer_ se aplica sobre los datos en crudo antes de introducirlos en el modelo, la estrategia de _sampling_ se aplica sobre los datos en crudo retornados por el modelo. Ninguno de los dos componentes pertenecen al modelo, pero se comentan por completitud en el análisis.\n",
    "\n",
    "El sistema es autoregresivo, lo que quiere decir que la salida alimenta su propia entrada.\n",
    "\n",
    "```\n",
    "Input ─> Embedding ─> 26 x Decoder ─> Norm ─> Logits -> Output\n",
    "      ↑                                                   │\n",
    "      └───────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Empieza con un _token_, tradicionalmente el _BOS_ (_Begin of Sentence_), genera un nuevo _token_, y lo inyecta de vuelta en el modelo. De esta forma la secuencia de salida va creciendo a cada iteración. El proceso se detiene al generar un _token_ determinado, tradicionalmente el _EOS_ (_End Of Sentence_), o al alcanzar un número máximo prefijado de _tokens_ de salida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
