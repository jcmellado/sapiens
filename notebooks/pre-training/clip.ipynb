{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbeff74-83d2-491a-af7a-24aaae718123",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "\n",
    "Revisión informal de la arquitectura _CLIP_ (_Contrastive Language-Image Pre-Training_).\n",
    "\n",
    "Modelo multimodal (lenguaje y visión) de código abierto publicado por OpenAI en enero de 2021.\n",
    "\n",
    "- [https://arxiv.org/pdf/2103.00020](https://arxiv.org/pdf/2103.00020)\n",
    "\n",
    "Aunque los términos arquitectura y modelo se utilicen indistintamente, es habitual presentar _CLIP_ como un método de _pre-entrenamiento_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa1b04-ad22-4cae-b406-36708fd8a503",
   "metadata": {},
   "source": [
    "## 1. Arquitectura\n",
    "\n",
    "El modelo utiliza dos _encoders_, uno para textos y otro para imágenes, cuyas salidas se comparan para medir su similitud.\n",
    "\n",
    "```\n",
    "  ┌────> Text Encoder ─────┐\n",
    "  |                        ↓\n",
    "Input                    Output\n",
    "  |                        ↑\n",
    "  └────> Image Encoder ────┘\n",
    "```\n",
    "\n",
    "La idea general es que ambos _encoders_ proyectan sus entradas a vectores dentro de un espacio de características alineado, lo que permite tratarlos de manera homogénea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c63d9-9780-45cf-b2d2-c9972fbc02c4",
   "metadata": {},
   "source": [
    "### 1.1. Funcionamiento General\n",
    "\n",
    "El modelo recibe dos tipos de entrada: textos e imágenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1dd71-764f-4e81-965a-df77e2fa1981",
   "metadata": {},
   "source": [
    "#### 1.1.1 Textos\n",
    "\n",
    "```\n",
    "Text ─> [SOT] Tokens [EOT] -> Embeddings -> Text Encoder -> Features\n",
    "```\n",
    "\n",
    "- Los textos se convierten en secuencias de _tokens_\n",
    "- Se añaden _tokens_ especiales de inicio _SOT_ y final _EOT_ de texto a las secuencias\n",
    "- Los _tokens_ se convierten en _embeddings_\n",
    "- Los _embeddings_ se procesan por el _encoder_ de texto para enriquecerlos con información de contexto aplicando el mecanismo de atención\n",
    "- Se toma el resultado de la salida del _encoder_ correspondiente al _token_ _EOT_\n",
    "- Y se calcula el vector de características (_features_) proyectando el resultado obtenido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111f9a4-5881-4276-8e0c-9cbcec8bbe19",
   "metadata": {},
   "source": [
    "#### 1.1.2. Imágenes\n",
    "\n",
    "```\n",
    "Image ─> Patches ─> [CLS] Embeddings -> Image Encoder -> Features\n",
    "```\n",
    "\n",
    "- Las imágenes se dividen en cuadrículas (_patches_)\n",
    "- Los _patches_ se convierten en secuencias de _embeddings_\n",
    "- Se añade un _embedding_ especial _CLS_ al inicio de las secuencias\n",
    "- Los _embeddings_ se procesan por el _encoder_ de imágenes para enriquecerlos con información de contexto aplicando el mecanismo de atención\n",
    "- Se toma el resultado de la salida del _encoder_ correspondiente al _embedding_ _CLS_\n",
    "- Y se calcula el vector de características (_features_) proyectando el resultado obtenido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4472d-104c-43ac-b7dd-f79a9e5898e9",
   "metadata": {},
   "source": [
    "#### 1.1.3. Similitud\n",
    "\n",
    "```\n",
    "Text ─> [SOT] Tokens [EOT] -> Embeddings -> Text Encoder -> Features ─┐\n",
    "                                                                      ↓\n",
    "                                                                  Similarity\n",
    "                                                                      ↑\n",
    "Image ─> Patches ─> [CLS] Embeddings -> Image Encoder -> Features ────┘\n",
    "```\n",
    "\n",
    "La salida del modelo es el resultado de aplicar la medida de _similitud coseno_ que mide la semejanza existente entre dos vectores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64baa0-681d-429a-9966-65011a24603e",
   "metadata": {},
   "source": [
    "### 1.2. Ejemplo\n",
    "\n",
    "Dada una lista de textos (clases):\n",
    "- $ \\text{Un libro en una mesa} $\n",
    "- $ \\text{La torre Eiffel} $\n",
    "- $ \\text{Un grupo de personas} $\n",
    "\n",
    "Y una imagen:\n",
    "- $ \\text{paris.png} $\n",
    "\n",
    "La salida del modelo puede utilizarse para obtener un vector con las probabilidades para cada clase:\n",
    "- $ \\begin{bmatrix} 0.1 & 0.7 & 0.2 \\end{bmatrix} $\n",
    "\n",
    "Este es un ejemplo de clasificación conocido como _zero-shot_, donde el modelo puede clasificar imágenes en clases con las que no fue explícitamente entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7826f-1946-4e3c-bf65-2fd60a7344e6",
   "metadata": {},
   "source": [
    "## 3. Text Encoder\n",
    "\n",
    "El _encoder_ de texto utiliza una arquitectura _Transformer_ en la que, en vez de proyectar la salida de la atención a una matriz de _scores_ con el tamaño del vocabulario, proyecta la salida de la atención correspondiente al _token_ _EOT_ a un vector de características.\n",
    "\n",
    "```\n",
    "Text ─> [SOT] Tokens [EOT] ─> Embeddings ─> PE ─> N x Transformer ─> Norm ─> Features\n",
    "```\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = self.token_embedding(text).type(self.dtype)\n",
    "x = x + self.positional_embedding.type(self.dtype)\n",
    "x = self.transformer(x)\n",
    "x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "# take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b0b5b-1421-43b7-81e4-837466ca18ab",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizer\n",
    "\n",
    "Cada texto de entrada se procesa por un _tokenizer_ que convierte texto libre de tamaño arbitrario en una secuencia de _tokens_ $T$.\n",
    "\n",
    "El _tokenizer_ final entrenado tiene un vocabulario de $V$ entradas, incluyendo los _tokens_ especiales de inicio (_SOT_) y final de texto (_EOT_), que se añaden a las secuencias de _tokens_ antes de introducirlas en el modelo.\n",
    "\n",
    "Las secuencias generadas se truncan a una longitud máxima prefijada de $n$ _tokens_ como tamaño máximo de la ventana de contexto, completando con ceros las secuencias de menor longitud a modo de _padding_.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ T \\in \\{0, 1, \\dots, V-1\\}^{1 \\times n} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "\n",
    "all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "\n",
    "torch.zeros(len(all_tokens), context_length, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab98e9-661e-4993-a15e-1c3edfd1472c",
   "metadata": {},
   "source": [
    "### 3.2. Embedding\n",
    "\n",
    "La primera capa del _encoder_ convierte los _tokens_ en _embeddings_.\n",
    "\n",
    "Su matriz de pesos $W$ tiene tantas filas $V$ como entradas tiene el vocabulario, y tantas columnas como dimensiones $d$ se deseen para los _embeddings_.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,d}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{V,1} & w_{V,2} & \\ldots & w_{V,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La matriz es inicializada con una distribución normal, y aprendida durante el entrenamiento.\n",
    "\n",
    "Los _embeddings_ se obtienen al indexar la matriz de pesos $W$ con cada _token_ de la secuencia $T$.\n",
    "\n",
    "- $ X = \\begin{bmatrix}\n",
    "W[T_1]\n",
    "\\\\ W[T_2]\n",
    "\\\\ \\vdots\n",
    "\\\\ W[T_n]\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,d}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La secuencia de _embeddings_ $X$ se propaga a través del _encoder_ sometiéndose a transformaciones funcionales en cada capa.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ T \\in \\{0, 1, \\dots, V-1\\}^{1 \\times n} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{V \\times d} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Huelga decir que el modelo funciona con _batches_, por lo que las dimensiones reales son (_tamaño de batch_, _longitud de secuencia_, _dimensiones de los embeddings_), pero por simplicidad en el análisis se omite.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "\n",
    "nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "\n",
    "x = self.token_embedding(text).type(self.dtype) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c9492-e435-4aac-8b98-891e4fd9606d",
   "metadata": {},
   "source": [
    "### 3.3. Positional Embedding\n",
    "\n",
    "Esta capa enriquece los _embeddings_ con información posicional.\n",
    "\n",
    "La información se añade a través de una matriz de pesos $W$, inicializada con una distribución normal, y aprendida durante el entrenamiento.\n",
    "\n",
    "Esta forma de trabajar se diferencia de otras técnicas habituales como _Positional Encoding_ (_PE_), o _Rotary Positional Embedding_ (_RoPE_), que aplican una función para calcular la información a añadir.\n",
    "\n",
    "La matriz de pesos tiene tantas filas $n$ como _tokens_ admite la ventana máxima de contexto, y tantas columnas como dimensiones $d$ tienen los _embeddings_.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,d}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{n,1} & w_{n,2} & \\ldots & w_{n,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Los _embeddings_ se enriquecen con la información posicional sumándoles la matriz de pesos $W$.\n",
    "\n",
    "- $ PE = X + W = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,d}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,d}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,d}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{n,1} & w_{n,2} & \\ldots & w_{n,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La salida de este paso son los _embeddings_ enriquecidos con la información posicional.\n",
    "\n",
    "- $ X \\leftarrow PE $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ PE \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "\n",
    "nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "x = x + self.positional_embedding.type(self.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191478f-000b-400c-976d-6e5b840dd53c",
   "metadata": {},
   "source": [
    "### 3.4. Transformer\n",
    "\n",
    "Esta capa añade información de contexto a los _embeddings_, ya enriquecidos con información posicional, haciéndolos pasar por $N$ _transformers_.\n",
    "\n",
    "Cada _transformer_ se compone de una capa de atención y otra de _feed forward_, con capas intermedias de normalización y conexión residual.\n",
    "\n",
    "```\n",
    "N x (─> Norm ─> MHA ─> Add ─> Norm ─> FF ─> Add)\n",
    "     │                  ↑  │                 ↑\n",
    "     └──────────────────┘  └─────────────────┘\n",
    "```\n",
    "\n",
    "La salida del primer _transformer_ alimenta la entrada del segundo _transformer_, la salida del segundo la entrada del tercero, y así sucesivamente.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_1 = LayerNorm(d_model)\n",
    "self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "self.ln_2 = LayerNorm(d_model)\n",
    "self.mlp = nn.Sequential(OrderedDict([\n",
    "  (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "  (\"gelu\", QuickGELU()),\n",
    "  (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "]))\n",
    "\n",
    "x = x + self.attention(self.ln_1(x))\n",
    "x = x + self.mlp(self.ln_2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa0d16-c387-4c12-b4bd-7586448c76a3",
   "metadata": {},
   "source": [
    "#### 3.4.1. Norm (Pre-Multi-Head Attention)\n",
    "\n",
    "Esta capa normaliza los _embeddings_ utilizando _Layer Normalization_.\n",
    "\n",
    "- [https://arxiv.org/pdf/1607.06450](https://arxiv.org/pdf/1607.06450)\n",
    "\n",
    "Esta función de normalización ajusta los valores de entrada restando la media y dividiendo por la raíz cuadrada de la varianza, más una constante $\\epsilon$ para evitar divisiones por cero, y aplicando una transformación afín con dos parámetros $\\gamma$ y $\\beta$ aprendidos durante el entrenamiento.\n",
    "\n",
    "- $ \\operatorname{LayerNorm}(x) = \\cfrac{x - \\mu[x]}{\\sqrt{\\sigma[x] + \\epsilon}} \\gamma + \\beta $\n",
    "\n",
    "Para el primer _transformer_, la función se aplica, fila a fila, a los _embeddings_ enriquecidos con información posicional.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{LayerNorm}(X_1)\n",
    "\\\\ \\operatorname{LayerNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{LayerNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Y para el resto de _transformers_, se calcula aplicando la función a la salida del _transformer_ anterior.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_1 = LayerNorm(d_model)\n",
    "\n",
    "self.ln_1(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15bfac-1ad3-48d9-98a5-efce4dedebaf",
   "metadata": {},
   "source": [
    "#### 3.4.2. Multi-Head Attention (MHA)\n",
    "\n",
    "Esta capa implementa el mecanismo de atención según se describe en el _paper_ seminal \"_Attention Is All You Need_\".\n",
    "\n",
    "- [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "Esta capa tiene $H$ cabezas de atención con sus correspondientes matrices de pesos $W_Q^h$, $W_K^h$ y $W_V^h$, aprendidas durante el entrenamiento.\n",
    "\n",
    "- $ d_k = d_v = \\cfrac{d}{H} $\n",
    "\n",
    "- $ W_Q^h \\in \\mathbb{R}^{d \\times d_k} $\n",
    " \n",
    "- $ W_K^h \\in \\mathbb{R}^{d \\times d_k} $\n",
    "  \n",
    "- $ W_V^h \\in \\mathbb{R}^{d \\times d_v} $\n",
    "\n",
    "Los _embeddings_ normalizados se proyectan por cada una de las matrices de pesos para obtener las matrices $Q^h$ (_Query_), $K^h$ (_Key_) y $V^h$ (_Value_).\n",
    "\n",
    "- $ Q^h = X W_Q^h $\n",
    "\n",
    "- $ K^h = X W_K^h $\n",
    "\n",
    "- $ V^h = X W_V^h $\n",
    "\n",
    "Y las proyecciones obtenidas se utilizan para calcular una matriz cuadrada de $n \\times n$ dimensiones.\n",
    "\n",
    "- $ S^h = \\operatorname{softmax}\\left(\\cfrac{Q^h {K^h}^T}{\\sqrt{d_k}}\\right) $\n",
    "\n",
    "Las matrices de pesos proyectan los _embeddings_ de entrada a tres espacios de características distintos.\n",
    "\n",
    "El producto escalar entre dos de las proyecciones compara dos de los espacios de características, valores altos indican alta similaridad.\n",
    "\n",
    "El resultado del producto se escala, y se aplica la función $softmax$ para convertir el resultado en una distribución de probabilidad.\n",
    "\n",
    "De forma que la matriz cuadrada resultante indica para cada _embedding_ que atención presta a si mismo y al resto de _embeddings_ de la secuencia.\n",
    "\n",
    "La matriz $V^h$ en el tercer espacio de características se pondera por la distribución de probabilidad para obtener las salidas de las cabezas de atención.\n",
    "\n",
    "- $ A^h = S^h V^h $\n",
    "\n",
    "Y finalmente, se concatenan todas las matrices de atención, y se proyectan por una matriz de pesos $W_O$ que retorna el resultado a las dimensiones de los _embeddings_.\n",
    "\n",
    "- $ X \\leftarrow \\operatorname{concat}(A^1, A^2 \\ldots, A^H) W_O $\n",
    "\n",
    "Aunque no se indica de manera explícita, se utilizan máscaras, para ignorar _tokens_ de _padding_, y para no calcular la atención de un _token_ respecto a _tokens_ posteriores aún no emitidos.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ Q^h \\in \\mathbb{R}^{n \\times d_k} $\n",
    "\n",
    "- $ K^h \\in \\mathbb{R}^{n \\times d_k} $\n",
    "\n",
    "- $ V^h \\in \\mathbb{R}^{n \\times d_v} $\n",
    "\n",
    "- $ A^h \\in \\mathbb{R}^{n \\times d_v} $\n",
    "\n",
    "- $ W_O \\in \\mathbb{R}^{H d_v \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "\n",
    "self.attention(self.ln_1(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dacaa5-5ac8-43d2-9661-04c6b8debf07",
   "metadata": {},
   "source": [
    "#### 3.4.3. Add (Post-Multi-Head Attention)\n",
    "\n",
    "Esta capa realiza una conexión residual para propagar información originalmente presente en la entrada del _transformer_.\n",
    "\n",
    "Para el primer _transformer_, se suma a la salida de la atención la entrada al _transformer_, que son los _embeddings_ enriquecidos con información posicional.\n",
    "\n",
    "- $ ATN = X + PE $\n",
    "\n",
    "Y para el resto de _transformers_, se suma a la salida de la atención del _transformer_ en curso la salida del _transformer_ anterior.\n",
    "\n",
    "La salida de este paso es el resultado de la suma.\n",
    "\n",
    "- $ X \\leftarrow ATN $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ P \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ A \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = x + self.attention(self.ln_1(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54954036-2065-49cb-8066-b89937a8d58e",
   "metadata": {},
   "source": [
    "#### 3.4.4. Norm (Pre-Feed Forward)\n",
    "\n",
    "Esta capa normaliza los _embeddings_ utilizando _Layer Normalization_.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{LayerNorm}(X_1)\n",
    "\\\\ \\operatorname{LayerNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{LayerNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_2 = LayerNorm(d_model)\n",
    "\n",
    "self.ln_2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aae333-a34c-4817-a586-64ea7b912506",
   "metadata": {},
   "source": [
    "#### 3.4.5. Feed Forward (FF)\n",
    "\n",
    "Esta capa añade no linealidad expandiendo las dimensiones internas del modelo para mejorar la potencia expresiva del mismo.\n",
    "\n",
    "Aplica una primera proyección lineal que expande cuatro veces las dimensiones del modelo.\n",
    "\n",
    "Aplica la función _GELU_ (_Gaussian Error Linear Units_) para añadir no linealidad.\n",
    "\n",
    "- [https://arxiv.org/pdf/1606.08415](https://arxiv.org/pdf/1606.08415)\n",
    "\n",
    "Y aplica una segunda proyección lineal para reducir el modelo a sus dimensiones originales.\n",
    "\n",
    "- $ X \\leftarrow \\operatorname{GELU}(X W_1) W_2 $\n",
    "\n",
    "Ambas matrices de pesos son aprendidas durante el entrenamiento.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ W_1 \\in \\mathbb{R}^{d \\times 4d} $\n",
    "\n",
    "- $ W_2 \\in \\mathbb{R}^{4d \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.mlp = nn.Sequential(OrderedDict([\n",
    "  (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "  (\"gelu\", QuickGELU()),\n",
    "  (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "]))\n",
    "\n",
    "self.mlp(self.ln_2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ffab8-dca6-4c46-ac85-87ff01531826",
   "metadata": {},
   "source": [
    "#### 3.4.6. Add (Post-Feed Forward)\n",
    "\n",
    "Esta capa realiza una conexión residual para propagar información originalmente presente antes del _feed forward_.\n",
    "\n",
    "Suma a la salida de la capa anterior la salida de la conexión residual aplicada tras la atención.\n",
    "\n",
    "- $ X \\leftarrow X + ATN $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ A \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = x + self.mlp(self.ln_2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7b9e9-b15e-4ec1-b7fd-b908f431df24",
   "metadata": {},
   "source": [
    "### 3.5. Norm (Final)\n",
    "\n",
    "Esta capa normaliza los _embeddings_ utilizando _Layer Normalization_.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{LayerNorm}(X_1)\n",
    "\\\\ \\operatorname{LayerNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{LayerNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "x = self.ln_final(x).type(self.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275e35a-e023-482c-8e0f-2a52c6c56787",
   "metadata": {},
   "source": [
    "### 3.6. Features\n",
    "\n",
    "Esta capa calcula la salida final del _encoder_ de texto.\n",
    "\n",
    "Proyecta un _embedding_ específico a un espacio de caracteristicas de $m$ dimensiones mediante una matriz $W$, inicializada con una distribución normal, y aprendida durante el entrenamiento.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,m}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,m}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{d,1} & w_{d,2} & \\ldots & w_{d,m}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La capa localiza la posición $k$ del _token_ _EOT_ dentro de la secuencia original $T$ de _tokens_ de entrada.\n",
    "\n",
    "- $ k = \\operatorname{lookup}(\\text{EOT}, T) $\n",
    "\n",
    "Toma el _embedding_ correspondiente a dicha posición de la secuencia de _embeddings_.\n",
    "\n",
    "- $ Y = X[k] $\n",
    "\n",
    "Y lo proyecta mediante la matriz de pesos $W$ para obtener el vector de caracteristicas $F$ de $m$ dimensiones.\n",
    "\n",
    "- $ F = Y W = \\begin{bmatrix}\n",
    "y_1 & y_2 & \\ldots & y_d\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,m}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,m}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{d,1} & w_{d,2} & \\ldots & w_{d,m}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "f_1 & f_2 & \\ldots & f_m\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ T \\in \\{0, 1, \\dots, V-1\\}^{1 \\times n} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ Y \\in \\mathbb{R}^{1 \\times d} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{d \\times m} $\n",
    "\n",
    "- $ F \\in \\mathbb{R}^{1 \\times m} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "\n",
    "nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "# take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652bbcc-00da-4b34-8374-549949c3c903",
   "metadata": {},
   "source": [
    "## 4. Image Encoder\n",
    "\n",
    "El _encoder_ de imágenes utiliza una arquitectura _Transformer_ en la que, en vez de proyectar la salida a una matriz de _scores_ con el tamaño del vocabulario, proyecta la salida correspondiente a un _emebedding_ especial _CLS_ a un vector de características.\n",
    "\n",
    "```\n",
    "Image ─> Patches ─> [CLS] Embeddings ─> PE ─> Norm ─> M x Transformer ─> Norm ─> Features\n",
    "```\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "x = self.conv1(x)\n",
    "x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1]), x], dim=1)\n",
    "x = x + self.positional_embedding.to(x.dtype)\n",
    "x = self.ln_pre(x)\n",
    "x = self.transformer(x)\n",
    "x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "x = x @ self.proj\n",
    "```\n",
    "\n",
    "Aclarar que el _paper_ de _CLIP_ propone dos arquitecturas para el _encoder_ de imágenes. Una basada en una arquitectura llamada _ResNet_. Y otra basada en _Vision Transformer_ (_ViT_), que es la que analiza aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4e341-4914-4b63-8ba3-9213de7e2995",
   "metadata": {},
   "source": [
    "### 4.1. Embeddings\n",
    "\n",
    "De igual forma que el texto libre se convierte en una secuencia de _tokens_ con un _tokenizer_, las imágenes se dividen en una secuencia de cuadriculas (_patches_) con una capa convolucional.\n",
    "\n",
    "Cada imagen de entrada $I$ se trata como un tensor de tamaño fijo $W \\times H$ con tres componentes de color (_RGB_).\n",
    "\n",
    "El número $p$ de _patches_ a generar depende de las dimensiones de la imagen y el tamaño $S \\times S$ de los _patches_.\n",
    "\n",
    "- $ p = \\cfrac{W}{S} \\times \\cfrac{H}{S} $\n",
    "\n",
    "El tamaño de _kernel_ y _stride_ de la capa convolucional se configuran con el mismo valor $S$ para dividir la imagen en cuadriculas que no se solapan entre sí y sin dejar espacios intermedios. \n",
    "\n",
    "Y la dimensión de salida de la capa convolucional se configura con un valor $d$ para que genere un vector de $d$ dimensiones por cada _patch_.\n",
    "\n",
    "Los $p$ vectores generados por la capa son el resultado de aplicar $d$ matrices de pesos (_kernels_), aprendidas durante el entrenamiento, a cada _patch_ de tamaño $3 \\times S \\times S$.\n",
    "\n",
    "Los vectores de $d$ dimensiones generados por la capa se interpretan directamente como _embeddings_. De forma que la salida de este paso es la secuencia de $p$ _embeddings_ generados a partir de la imagen $I$ de entrada.\n",
    "\n",
    "- $ X = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,d}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{p,1} & x_{p,2} & \\ldots & x_{p,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La secuencia de _embeddings_ $X$ se propaga a través del _encoder_ sometiéndose a transformaciones funcionales en cada capa.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ I \\in \\mathbb{R}^{3 \\times W \\times H} $\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{p \\times d} $\n",
    "\n",
    "Huelga decir que el modelo funciona con _batches_, por lo que las dimensiones reales son (_tamaño de batch_, _longitud de secuencia_, _dimensiones de los embeddings_), pero por simplicidad en el análisis se omite.\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c70f5c-a62b-4409-8291-c1e2fb41b908",
   "metadata": {},
   "source": [
    "### 4.2. Class Embedding\n",
    "\n",
    "A la secuencia de _embeddings_ obtenida en la capa anterior se le antepone un _embedding_ especial _CLS_ (_Class Embedding_).\n",
    "\n",
    "Este _embedding_ es un vector de $d$ dimensiones aprendido por el modelo, no un valor fijo predefinido como los _tokens_ _SOT_ y _EOT_ utilizados en el _encoder_ de texto.\n",
    "\n",
    "- $ \\text{CLS} = \\begin{bmatrix}\n",
    "\\text{cls}_1 & \\text{cls}_2 & \\ldots & \\text{cls}_d\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Se inicializa con valores tomados de una distribución normal, y se actualiza durante el entrenamiento.\n",
    "\n",
    "La salida de este paso es la secuencia de $n = p + 1$ _embeddings_ resultante de añadir el _embedding_ especial _CLS_ al inicio de la secuencia $X$.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\text{cls}_1 & \\text{cls}_2 & \\ldots & \\text{cls}_d\n",
    "\\\\ x_{1,1} & x_{1,2} & \\ldots & x_{1,d}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{p,1} & x_{p,2} & \\ldots & x_{p,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X_{in} \\in \\mathbb{R}^{p \\times d} $\n",
    "\n",
    "- $ \\text{CLS} \\in \\mathbb{R}^{1 \\times d} $\n",
    "\n",
    "- $ X_{out} \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "scale = width ** -0.5\n",
    "self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "\n",
    "x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c2304-1c94-4b77-b6e3-d173e6c3256f",
   "metadata": {},
   "source": [
    "### 4.3. Positional Embedding\n",
    "\n",
    "Esta capa enriquece los _embeddings_ con información posicional de igual forma que se hace en el _encoder_ de texto.\n",
    "\n",
    "La información se añade a través de una matriz de pesos $W$, inicializada con una distribución normal, y aprendida durante el entrenamiento.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,d}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{n,1} & w_{n,2} & \\ldots & w_{n,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La salida de esta capa se obtiene sumando a la secuencia de _embeddings_ la matriz de pesos $W$.\n",
    "\n",
    "- $ X \\leftarrow X + W = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,d}\n",
    "\\\\ x_{2,1} & x_{2,2} & \\ldots & x_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ x_{n,1} & x_{n,2} & \\ldots & x_{n,d}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,d}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,d}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{n,1} & w_{n,2} & \\ldots & w_{n,d}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "scale = width ** -0.5\n",
    "self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "\n",
    "x = x + self.positional_embedding.to(x.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4a7e4-1926-42bd-bf0f-e9f8537c05b0",
   "metadata": {},
   "source": [
    "### 4.4. Norm (Pre-Transformer)\n",
    "\n",
    "Esta capa normaliza los _embeddings_ utilizando _Layer Normalization_ de igual forma que otras capas de normalización del modelo.\n",
    "\n",
    "- $ X \\leftarrow \\begin{bmatrix}\n",
    "\\operatorname{LayerNorm}(X_1)\n",
    "\\\\ \\operatorname{LayerNorm}(X_2)\n",
    "\\\\ \\vdots\n",
    "\\\\ \\operatorname{LayerNorm}(X_n)\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_pre = LayerNorm(width)\n",
    "\n",
    "x = self.ln_pre(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfba32-4fcb-4472-9977-2e7b67c9beee",
   "metadata": {},
   "source": [
    "### 4.5. Transformer\n",
    "\n",
    "Esta capa añade información de contexto a los _embeddings_, ya enriquecidos con información posicional, haciéndolos pasar por $M$ _transformers_.\n",
    "\n",
    "Cada _transformer_ se compone de una capa de atención y otra de _feed forward_, con capas intermedias de normalización y conexión residual.\n",
    "\n",
    "```\n",
    "M x (─> Norm ─> MHA ─> Add ─> Norm ─> FF ─> Add)\n",
    "     │                  ↑  │                 ↑\n",
    "     └──────────────────┘  └─────────────────┘\n",
    "```\n",
    "\n",
    "La salida del primer _transformer_ alimenta la entrada del segundo _transformer_, la salida del segundo la entrada del tercero, y así sucesivamente.\n",
    "\n",
    "Sigue el mismo proceso que el descrito para el _encoder_ de texto, enriqueciendo los _embeddings_ $X$ con información de contexto.\n",
    "\n",
    "La salida de esta capa es la salida final del último _transformer_.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_1 = LayerNorm(d_model)\n",
    "self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "self.ln_2 = LayerNorm(d_model)\n",
    "self.mlp = nn.Sequential(OrderedDict([\n",
    "  (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "  (\"gelu\", QuickGELU()),\n",
    "  (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "]))\n",
    "\n",
    "x = x + self.attention(self.ln_1(x))\n",
    "x = x + self.mlp(self.ln_2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd14818-3a18-4e9c-96b4-b4dcebc8df37",
   "metadata": {},
   "source": [
    "### 4.6. Norm (Post-Transformer)\n",
    "\n",
    "Esta capa normaliza los _embeddings_ aplicando _Layer Normalization_, pero sólo sobre el primer elemento, que se corresponde con el _embedding_ especial _CLS_.\n",
    "\n",
    "- $ Y = \\operatorname{LayerNorm}(X_1) $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ X \\in \\mathbb{R}^{n \\times d} $\n",
    "\n",
    "- $ Y \\in \\mathbb{R}^{1 \\times d} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.ln_post = LayerNorm(width)\n",
    "\n",
    "x = self.ln_post(x[:, 0, :])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad5c1b-b4cf-4495-bd3e-fbc8baf4a672",
   "metadata": {},
   "source": [
    "### 4.7. Features\n",
    "\n",
    "Esta capa calcula la salida final del _encoder_ de imagen.\n",
    "\n",
    "Proyecta el _embedding_ especial _CLS_ a un espacio de caracteristicas de $m$ dimensiones mediante una matriz $W$, inicializada con una distribución normal, y aprendida durante el entrenamiento.\n",
    "\n",
    "- $ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,m}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,m}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{d,1} & w_{d,2} & \\ldots & w_{d,m}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "La capa proyecta el _embedding_ por la matriz $W$ para obtener el vector de caracteristicas $F$ de $m$ dimensiones.\n",
    "\n",
    "- $ F = Y W = \\begin{bmatrix}\n",
    "y_1 & y_2 & \\ldots & y_d\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\ldots & w_{1,m}\n",
    "\\\\ w_{2,1} & w_{2,2} & \\ldots & w_{2,m}\n",
    "\\\\ \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\ w_{d,1} & w_{d,2} & \\ldots & w_{d,m}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "f_1 & f_2 & \\ldots & f_m\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ Y \\in \\mathbb{R}^{1 \\times d} $\n",
    "\n",
    "- $ W \\in \\mathbb{R}^{d \\times m} $\n",
    "\n",
    "- $ F \\in \\mathbb{R}^{1 \\times m} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "scale = width ** -0.5\n",
    "self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "x = x @ self.proj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903afa0-b55a-4554-931c-ba69de5deeaf",
   "metadata": {},
   "source": [
    "## 5. Similarity\n",
    "\n",
    "Los vectores de características obtenidos por los _encoders_ se comparan utilizando la _similitud coseno_ (_Cosine Similarity_).\n",
    "\n",
    "```\n",
    "Text ─> Text Encoder ─────┐\n",
    "                          ↓\n",
    "                 Cosine Similarity ─> Logits\n",
    "                          ↑\n",
    "Image ──> Image Encoder ──┘\n",
    "```\n",
    "\n",
    "La entrada de este paso son los vectores de características resultantes de aplicar los _encoders_ a las entradas del modelo.\n",
    "\n",
    "- $ F_T = \\operatorname{encode}(\\text{text}) $\n",
    "\n",
    "- $ F_I = \\operatorname{encode}(\\text{image}) $\n",
    "\n",
    "Y la salida son los conocidos como \"_logits_\". Es decir, las puntuaciones brutas (_scores_) resultantes de comparar los dos vectores de características. Es habitual aplicar a la salida del modelo la función $softmax$ para convertir las puntuaciones en una distribución de probabilidad con valores entre 0 y 1.\n",
    "\n",
    "Para un ejemplo de clasificación de tipo _zero-shot_, a la entrada se tiene una imagen y varios textos, y a su salida una matriz de _scores_ en vez de un único valor. Por lo que, por claridad en el cálculo de la salida del modelo en este apartado, se supone que la entrada al modelo es un _batch_ de $N$ imágenes y $M$ secuencias de _tokens_.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ I \\in \\mathbb{R}^{N \\times 3 \\times W \\times H} $\n",
    "\n",
    "- $ T \\in \\mathbb{R}^{M \\times n} $\n",
    "\n",
    "- $ F_I \\in \\mathbb{R}^{N \\times m} $\n",
    "\n",
    "- $ F_T \\in \\mathbb{R}^{M \\times m} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "image_features = self.encode_image(image)\n",
    "text_features = self.encode_text(text)\n",
    "\n",
    "image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "logit_scale = self.logit_scale.exp()\n",
    "logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "logits_per_text = logits_per_image.t()\n",
    "\n",
    "# shape = [global_batch_size, global_batch_size]\n",
    "return logits_per_image, logits_per_text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2965982-3a8f-4e05-a91d-5745072a347d",
   "metadata": {},
   "source": [
    "### 5.1. Cosine Similarity\n",
    "\n",
    "La _similitud coseno_ se calcula como el producto escalar entre dos vectores unitarios.\n",
    "\n",
    "El producto escalar es una medida de la semejanza de dos vectores, a mayor semejanza, mayor magnitud.\n",
    "\n",
    "- $ a \\cdot b = \\lVert a \\rVert \\lVert b \\rVert \\cos{\\theta} $\n",
    "\n",
    "Si los vectores son unitarios entonces el producto escalar es directamente el coseno del ángulo $\\theta$ que forman los dos vectores entre sí.\n",
    "\n",
    "- $ a \\cdot b = \\cos{\\theta} $\n",
    "\n",
    "Y el producto escalar se calcula como la suma de los productos de los componentes de los dos vectores, uno a uno.\n",
    "\n",
    "- $  a \\cdot b = \\sum\\limits_{i=1}^{n} a_i b_i $\n",
    "\n",
    "El modelo utiliza específicamente la norma _L2_ para calcular la longitud de los vectores y convertirlos en unitarios.\n",
    "\n",
    "- $ \\lVert x \\rVert_2 = \\sqrt{\\sum\\limits_{i=1}^{n} x_i^2} $\n",
    "\n",
    "La salida de este paso es la _similitud coseno_ calculada como el producto de los vectores de características normalizados.\n",
    "\n",
    "- $ S = \\cfrac{F_I \\cdot F_T}{\\lVert F_I \\rVert_2 \\lVert F_T \\rVert_2} $\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ F_I \\in \\mathbb{R}^{N \\times m} $\n",
    "\n",
    "- $ F_T \\in \\mathbb{R}^{M \\times m} $\n",
    "\n",
    "- $ S \\in \\mathbb{R}^{N \\times M} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "logit_scale * image_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db9dad-60d1-4100-82e1-7ebe71820894",
   "metadata": {},
   "source": [
    "### 5.2. Logits\n",
    "\n",
    "Los _logits_ son las puntuaciones finales de similitud entre las representaciones de las imágenes y los textos, y viceversa.\n",
    "\n",
    "Se obtienen a partir de la _similitud coseno_ escalada por la exponencial de un factor de _temperatura_ $\\tau$ aprendido durante el entrenamiento.\n",
    "\n",
    "- $ S_I = e^{\\tau} S $\n",
    "\n",
    "Cada elemento $(i, j)$ de esta matriz representa la similitud (_logit_) entre la imagen $i$-ésima y el texto $j-$ésimo.\n",
    "\n",
    "Los _logits_ de los textos frente a las imágenes son la traspuesta de la matriz anterior.\n",
    "\n",
    "- $ S_T = S_I^T $\n",
    "\n",
    "Cada elemento $(i, j)$ de esta matriz representa la similitud (_logit_) entre el texto $i$-ésimo y la imagen $j-$ésima.\n",
    "\n",
    "Dimensiones:\n",
    "\n",
    "- $ S \\in \\mathbb{R}^{N \\times M} $\n",
    "\n",
    "- $ \\tau \\in \\mathbb{R} $\n",
    "\n",
    "- $ S_I \\in \\mathbb{R}^{N \\times M} $\n",
    "\n",
    "- $ S_T \\in \\mathbb{R}^{M \\times N} $\n",
    "\n",
    "Algunas líneas relevantes del código fuente:\n",
    "\n",
    "```python\n",
    "self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "logit_scale = self.logit_scale.exp()\n",
    "logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "logits_per_text = logits_per_image.t()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21020001-4f1f-45ec-b843-0b413d1aa67d",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento\n",
    "\n",
    "El hecho de que los dos _encoders_ del modelo generen vectores con las mismas dimensiones no quiere decir que puedan interpretarse como pertenecientes a un mismo espacio de características.\n",
    "\n",
    "Un determinado valor numérico en una determinada dimensión de un vector de características generado para un texto no tiene porque tener la misma interpretación _semántica_ que el mismo valor en la misma dimensión de un vector de características generado para una imagen.\n",
    "\n",
    "Es el entrenamiento del modelo el que garantiza que los vectores generados por el modelo sean los más similares posible para textos e imágenes relacionados, y menos similares para textos e imágenes no relacionadas.\n",
    "\n",
    "En el repositorio del modelo no se proporciona el código de entrenamiento, pero en el _paper_ hay pseudo-código.\n",
    "\n",
    "```python\n",
    "labels = np.arange(n)\n",
    "\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\n",
    "loss_t = cross_entropy_loss(logits, labels, axis=1)\n",
    "\n",
    "loss = (loss_i + loss_t)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a463112-e6eb-455d-8d50-9868457dc2d1",
   "metadata": {},
   "source": [
    "### 6.1. Cross Entropy Loss\n",
    "\n",
    "Para el entrenamiento se utiliza la función de pérdida de entropía cruzada (_Cross Entropy Loss_) para $N$ muestras y $C$ clases posibles.\n",
    "\n",
    "- $ L = -\\cfrac{1}{N} \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{C} y_{i,j} ln\\left(\\cfrac{e^{x_{i,j}}}{\\sum\\limits_{k=1}^{C} e^{x_{i,k}}}\\right) $\n",
    "\n",
    "Esta función primero aplica la función $softmax$ en una de las dimensiones para convertir las puntuaciones brutas (_logits_) $x$ obtenidas para cada muestra en una distribución de probabilidad.\n",
    "\n",
    "- $ \\operatorname{softmax}(x_i)_j = \\cfrac{e^{x_{i,j}}}{\\sum\\limits_{k=1}^{C} e^{x_{i,k}}} $\n",
    "\n",
    "Y luego calcula el logaritmo con signo negativo para penalizar los errores.\n",
    "\n",
    "- $ -\\ln(\\operatorname{softmax}(x_i)_j) $\n",
    "\n",
    "Como la salida de la función $softmax$ son valores entre $0$ y $1$, donde un valor cercano a $0$ significa que la predicción es incorrecta, y cercano a $1$ que es correcta, el logaritmo con signo negativo obtiene un valor de pérdida bajo cuando la probabilidad es alta, y alto cuando la probabilidad es baja.\n",
    "\n",
    "- $ -\\ln(0) = \\infty $\n",
    "\n",
    "- $ -\\ln(1) = 0 $\n",
    "\n",
    "La pérdida para cada una de las muestra se realiza calculando el producto por las etiquetas $y$, que normalmente se expresan mediante un vector _one-hot_, donde sólo un elemento es 1 y el resto es 0, por lo que todos los productos se anulan excepto el correspondiente a la posición $j = t$ que contiene la clase correcta (_ground truth_).\n",
    "\n",
    "- $ l_i = -y_{i,t} \\ln(\\operatorname{softmax}(x_i)_t) $\n",
    "\n",
    "La pérdida final se obtiene como un único escalar calculando la media de todas las pérdidas individuales obtenidas.\n",
    "\n",
    "- $ L = \\cfrac{1}{N} \\sum\\limits_{i=1}^{N} l_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8ff1c-2125-4be9-9a91-ffa26bf1d88b",
   "metadata": {},
   "source": [
    "### 6.2. Aprendizaje Contrastivo\n",
    "\n",
    "El entrenamiento se plantea como un problema de clasificación, donde los textos representan las clases en las que clasificar las imágenes, y viceversa.\n",
    "\n",
    "El modelo se entrena con _batches_ de $N$ imágenes y $N$ textos, por lo que la salida del modelo son matrices cuadradas de _logits_ de $N \\times N$ dimensiones.\n",
    "\n",
    "La función de pérdida de entropía cruzada aplica la función $softmax$ a lo largo de una dimensión, por lo que su salida sigue siendo una matriz de $N \\times N$ dimensiones.\n",
    "\n",
    "Y, conceptualmente, el logaritmo negativo se aplica a los elementos de la diagonal, ya que para cada imagen $i$ la predicción correcta es el texto $i$, y viceversa, por lo que se obtienen $N$ pérdidas individuales.\n",
    "\n",
    "La pérdida final se obtiene como un único escalar calculando la media de todos las pérdidas individuales obtenidas.\n",
    "\n",
    "Este proceso se aplica dos veces, una sobre los _logits_ de las imágenes, y otra sobre los de los textos.\n",
    "\n",
    "- $ l_I = L(S_I) $\n",
    "\n",
    "- $ l_T = L(S_T) $\n",
    "\n",
    "Y se calcula la pérdida total como la media de las dos pérdidas anteriores, para que el modelo aprenda a alinear los vectores de características en ambas direcciones.\n",
    "\n",
    "- $ l = \\cfrac{l_I + l_T}{2} $\n",
    "\n",
    "El modelo aprende a generar puntuaciones brutas más elevadas sobre la diagonal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
