{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a1411d-bece-452a-a95d-6141189d05d3",
   "metadata": {},
   "source": [
    "# Transformers - Implementación\n",
    "\n",
    "Implementación de la arquitectura _Transformer_ propuesta originalmente en el _paper_ seminal \"_Attention is All Your Need_\".\n",
    "\n",
    "- [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "La implementación se realiza capa a capa en _PyTorch_, lo que hoy en día no es práctico ni necesario, pero resulta útil para afianzar conceptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1af67-0dc8-4df0-a058-97f6c44d0b8c",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Para ejecutar este _notebook_ en local se requiere Python, y se recomienda crear un entorno virtual para instalar las dependencias de manera controlada.\n",
    "\n",
    "```\n",
    "uv venv --python=python3.12\n",
    "source .venv/bin/activate\n",
    "\n",
    "uv pip install torch==2.6.0\n",
    "\n",
    "uv pip install jupyterlab ipywidgets\n",
    "uv run jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a159f38-6fd3-49d4-8d69-c0a8c307563d",
   "metadata": {},
   "source": [
    "## 2. Modelo\n",
    "\n",
    "La implementación se realiza siguiendo la arquitectura original propuesta en el _paper_.\n",
    "\n",
    "- [https://github.com/jcmellado/sapiens/blob/main/notebooks/transformers/transformers.ipynb](https://github.com/jcmellado/sapiens/blob/main/notebooks/transformers/transformers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30e02a-ba49-4208-bb36-cb8091439cb0",
   "metadata": {},
   "source": [
    "### 2.1. Positional Encoding\n",
    "\n",
    "La capa de _Positional Encoding_ se implementa desde cero ya que _PyTorch_ no la proporciona de forma nativa.\n",
    "\n",
    "```\n",
    "In ─> Positional Encoding ─> Dropout ─> Out\n",
    "```\n",
    "\n",
    "Recordando la expresión de cálculo de _Positional Encoding_:\n",
    "\n",
    "- $ \\operatorname{PE}(i, j) = \\begin{cases} \n",
    "\\sin\\left( \\dfrac{i}{10000^{j/d}} \\right), & \\text{si } j \\text{ es par} \\\\\n",
    "\\cos\\left( \\dfrac{i}{10000^{(j-1)/d}} \\right), & \\text{si } j \\text{ es impar}\n",
    "\\end{cases} $\n",
    "\n",
    "La implementación utilizada para calcular los denominadores es un tanto distinta de la propuesta en el _paper_ original, por cuestiones de precisión numérica, pero equivalente.\n",
    "\n",
    "- $ e^{-a \\ln(b)} = \\cfrac{1}{e^{a \\ln(b)}} = \\cfrac{1}{e^{\\ln(b^a)}} = \\cfrac{1}{b^a} $\n",
    "\n",
    "- $ e^{-\\frac{j}{d} \\ln(10000)} = \\cfrac{1}{e^{\\frac{j}{d} \\ln(10000)}} = \\cfrac{1}{e^{\\ln(10000^{j/d})}} = \\cfrac{1}{10000^{j/d}} $\n",
    "\n",
    "La tabla con los valores calculados se registra como un _buffer_ para que durante el entrenamiento se trate como un parámetro y no se inicialice ni actualice.\n",
    "\n",
    "La capa de _dropout_ es utilizada habitualmente para prevenir el _overfitting_ y es mencionada en el _paper_ original. Aunque en algunas arquitecturas más recientes estas capas se están eliminando de los modelos en algunos casos.\n",
    "\n",
    "Por otra parte, en desarrollos actuales es común utilizar una técnica alternativa llamada _Rotary Positional Embeddings_ (RoPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd14b96-6463-4ff0-bf01-fe87c4ccaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim_embeddings: int,\n",
    "    max_sequence_length: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    position = torch.arange(0, max_sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim_embeddings, 2).float() * (-math.log(10000.0) / dim_embeddings))\n",
    "\n",
    "    pe = torch.zeros(max_sequence_length, dim_embeddings)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "\n",
    "    self.register_buffer('positional_encoding_table', pe)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    positional_encoding = self.positional_encoding_table[:, :embeddings.size(1), :]\n",
    "\n",
    "    return self.dropout(embeddings + positional_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbbee0-278c-4319-b5f4-d15bc27ca172",
   "metadata": {},
   "source": [
    "### 2.2. Feed Forward\n",
    "\n",
    "La capa de _Feed Forward_ se implementa como un módulo de forma independiente para poder utilizarlo en el _encoder_ y el _decoder_.\n",
    "\n",
    "```\n",
    "In ─> Linear ─> ReLU ─> Dropout ─> Linear ─> Out\n",
    "```\n",
    "\n",
    "Se utiliza la función de activación ReLU siguiendo el _paper_ original, aunque en arquitecturas más recientes es habitual utilizar otras funciones.\n",
    "\n",
    "Se aplica una capa de _dropout_ a la salida de la función de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665d9b41-623e-4d09-8e6d-ef378084f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim_model: int,\n",
    "    dim_expand: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.feed_forward = nn.Sequential(\n",
    "      nn.Linear(in_features=dim_model, out_features=dim_expand),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(p=dropout),\n",
    "      nn.Linear(in_features=dim_expand, out_features=dim_model)\n",
    "    )\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528a2e7-4e25-4d56-8710-6aaaee1b9e97",
   "metadata": {},
   "source": [
    "### 2.3. Encoder\n",
    "\n",
    "La implementación del _encoder_ se realiza siguiendo el diseño del _paper_ original.\n",
    "\n",
    "```\n",
    "In ─> MHA ─> Dropout ─> Add ─> Norm ─> FF ─> Dropout ─> Add ─> Norm ─> Out\n",
    "   │                     ↑          │                    ↑\n",
    "   └─────────────────────┘          └────────────────────┘\n",
    "```\n",
    "\n",
    "El parámetro `batch_first` se utiliza para que la atención se retorne con las dimensiones en el orden esperado (_batch_, _secuencia_, _embedding_).\n",
    "\n",
    "El parámetro `need_weights` se utiliza para que _PyTorch_ no retorne la matriz de pesos al calcular la atención, lo que aumenta el rendimiento.\n",
    "\n",
    "Se aplican capas de _dropout_ a la salida de cada capa.\n",
    "\n",
    "Las capas de normalización se aplican después de las conexiones residuales, pero en arquitecturas más recientes es habitual aplicarlas antes de las capas de atención y _feed forward_.\n",
    "\n",
    "Al entrenar el modelo con _batches_, cada secuencia del _batch_ puede tener una longitud distinta, por lo que se proporciona una matriz de _padding_ que enmascara los _tokens_ del final de cada secuencia para que tengan la misma longitud que la secuencia más larga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e302d75d-b1e1-4cd1-8521-c9a9ec53a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim_embeddings: int,\n",
    "    num_heads: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head_attention = nn.MultiheadAttention(\n",
    "        embed_dim=dim_embeddings, num_heads=num_heads, batch_first=True)\n",
    "    self.dropout_1 = nn.Dropout(p=dropout)\n",
    "    self.norm_1 = nn.LayerNorm(normalized_shape=dim_embeddings)\n",
    "\n",
    "    self.feed_forward = FeedForward(dim_model=dim_embeddings, dim_expand=dim_feedforward, dropout=dropout)\n",
    "    self.dropout_2 = nn.Dropout(p=dropout)\n",
    "    self.norm_2 = nn.LayerNorm(normalized_shape=dim_embeddings)\n",
    "\n",
    "  def forward(self, embeddings_pe: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "    attention, _ = self.multi_head_attention(\n",
    "        query=embeddings_pe, key=embeddings_pe, value=embeddings_pe, key_padding_mask=padding_mask, need_weights=False)\n",
    "    dropout_1 = self.dropout_1(attention)\n",
    "    residual_1 = embeddings_pe + dropout_1\n",
    "    norm_1 = self.norm_1(residual_1)\n",
    "\n",
    "    feedforward = self.feed_forward(norm_1)\n",
    "    dropout_2 = self.dropout_2(feedforward)\n",
    "    residual_2 = norm_1 + dropout_2\n",
    "    output = self.norm_2(residual_2)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fca848-e662-42f0-b2bc-6074f65cde9d",
   "metadata": {},
   "source": [
    "### 2.4. Encoder Stack\n",
    "\n",
    "La pila de _encoders_ se implementa como un módulo de _PyTorch_.\n",
    "\n",
    "La capa de _embeddings_ es compartida con el _decoder_, por lo que se pasa como parámetro en el constructor. Siguiendo las indicaciones del _paper_ original, la salida de esta capa se multiplica por la raíz cuadrada de las dimensiones de los _embeddings_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08251282-fa8a-4b8d-9e0f-55204963c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    num_encoders: int,\n",
    "    embedding: nn.Module,\n",
    "    dim_embeddings: int,\n",
    "    max_sequence_length: int,\n",
    "    num_heads: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = embedding\n",
    "    self.dim_embeddings = dim_embeddings\n",
    "    self.positional_encoding = PositionalEncoding(\n",
    "      dim_embeddings=dim_embeddings, max_sequence_length=max_sequence_length, dropout=dropout)\n",
    "\n",
    "    encoders = [\n",
    "      Encoder(dim_embeddings=dim_embeddings, num_heads=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "      for _ in range(num_encoders)\n",
    "    ]\n",
    "    self.encoders = nn.ModuleList(encoders)\n",
    "\n",
    "  def forward(self, tokens: torch.Tensor, padding_mask: torch.Tensor) -> torch.Tensor:\n",
    "    embeddings = self.embedding(tokens) * math.sqrt(self.dim_embeddings)\n",
    "    embeddings_pe = self.positional_encoding(embeddings)\n",
    "\n",
    "    output = embeddings_pe\n",
    "\n",
    "    for encoder in self.encoders:\n",
    "      output = encoder(output, padding_mask)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f86e53-cf5e-42a7-b4b8-6501bec55735",
   "metadata": {},
   "source": [
    "### 2.5. Decoder\n",
    "\n",
    "El _decoder_ se implementa siguiendo el diseño del _paper_ original.\n",
    "\n",
    "```\n",
    "In ─> MHA ─> DO ─> Add ─> Norm ─> MHA ─> DO ─> Add ─> Norm ─> FF ─> DO ─> Add ─> Norm ─> Out\n",
    "   │                ↑          │                ↑          │               ↑\n",
    "   └────────────────┘          └────────────────┘          └───────────────┘\n",
    "```\n",
    "\n",
    "Se crea una máscara de atención `attn_mask` para no tener en cuenta los elementos correspondientes a los _tokens_ de la secuencia que aún no se han generado.\n",
    "\n",
    "- $ \\begin{bmatrix}\n",
    "\\text{false} & \\text{true} & \\text{true} & \\ldots & \\text{true}\n",
    "\\\\ \\text{false} & \\text{false} & \\text{true} & \\ldots & \\text{true}\n",
    "\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\text{true}\n",
    "\\\\ \\text{false} & \\text{false} & \\text{false} & \\ldots & \\text{false}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Al entrenar el modelo con _batches_, cada secuencia del _batch_ puede tener una longitud distinta, por lo que se proporcionan matrices de _padding_ que enmascaran los _tokens_ del final de cada secuencia para que tengan la misma longitud que la secuencia más larga.\n",
    "\n",
    "El resto de parámetros son similares a los del _encoder_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d165df-85dd-4408-b288-8a516ecfdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim_embeddings: int,\n",
    "    num_heads: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.masked_attention = nn.MultiheadAttention(\n",
    "        embed_dim=dim_embeddings, num_heads=num_heads, batch_first=True)\n",
    "    self.dropout_1 = nn.Dropout(p=dropout)\n",
    "    self.norm_1 = nn.LayerNorm(normalized_shape=dim_embeddings)\n",
    " \n",
    "    self.attention = nn.MultiheadAttention(\n",
    "        embed_dim=dim_embeddings, num_heads=num_heads, batch_first=True)\n",
    "    self.dropout_2 = nn.Dropout(p=dropout)\n",
    "    self.norm_2 = nn.LayerNorm(normalized_shape=dim_embeddings)\n",
    " \n",
    "    self.feed_forward = FeedForward(dim_model=dim_embeddings, dim_expand=dim_feedforward, dropout=dropout)\n",
    "    self.dropout_3 = nn.Dropout(p=dropout)\n",
    "    self.norm_3 = nn.LayerNorm(normalized_shape=dim_embeddings)\n",
    "\n",
    "  def forward(self, embeddings_pe: torch.Tensor, decoder_padding_mask: torch.Tensor,\n",
    "              context: torch.Tensor, encoder_padding_mask: torch.Tensor\n",
    "  ) -> torch.Tensor:\n",
    "    sequence_len = embeddings_pe.size(1)\n",
    "    attn_mask = torch.triu(torch.ones(sequence_len, sequence_len, dtype=torch.bool), diagonal=1).to(embeddings_pe.device)\n",
    "      \n",
    "    self_attention, _ = self.masked_attention(\n",
    "        query=embeddings_pe, key=embeddings_pe, value=embeddings_pe, attn_mask=attn_mask, is_causal=True,\n",
    "        key_padding_mask=decoder_padding_mask, need_weights=False)\n",
    "    dropout_1 = self.dropout_1(self_attention)\n",
    "    residual_1 = embeddings_pe + dropout_1\n",
    "    norm_1 = self.norm_1(residual_1)\n",
    "\n",
    "    cross_attention, _ = self.attention(\n",
    "        query=norm_1, key=context, value=context,\n",
    "        key_padding_mask=encoder_padding_mask, need_weights=False)\n",
    "    dropout_2 = self.dropout_2(cross_attention)\n",
    "    residual_2 = norm_1 + dropout_2\n",
    "    norm_2 = self.norm_2(residual_2)\n",
    "\n",
    "    feedforward = self.feed_forward(norm_2)\n",
    "    dropout_3 = self.dropout_3(feedforward)\n",
    "    residual_3 = norm_2 + dropout_3\n",
    "    output = self.norm_3(residual_3)\n",
    " \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4fefd-8904-48e4-8400-fd48874c6671",
   "metadata": {},
   "source": [
    "### 2.6. Decoder Stack\n",
    "\n",
    "La pila de _decoders_ se implementa como un módulo.\n",
    "\n",
    "La capa de _embeddings_ es compartida con el _encoder_, por lo que se pasa como parámetro en el constructor. Siguiendo las indicaciones del _paper_ original, la salida de esta capa se multiplica por la raíz cuadrada de las dimensiones de los _embeddings_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6029d32-962e-4b03-a6a4-6a48c19f2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    num_decoders: int,\n",
    "    embedding: nn.Module,\n",
    "    dim_embeddings: int,\n",
    "    max_sequence_length: int,\n",
    "    num_heads: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding = embedding\n",
    "    self.dim_embeddings = dim_embeddings\n",
    "    self.positional_encoding = PositionalEncoding(\n",
    "      dim_embeddings=dim_embeddings, max_sequence_length=max_sequence_length, dropout=dropout\n",
    "    )\n",
    "\n",
    "    decoders = [\n",
    "      Decoder(dim_embeddings=dim_embeddings, num_heads=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        for _ in range(num_decoders)\n",
    "    ]\n",
    "    self.decoders = nn.ModuleList(decoders)\n",
    "\n",
    "  def forward(self, tokens: torch.Tensor, decoder_padding_mask: torch.Tensor,\n",
    "              context: torch.Tensor, encoder_padding_mask: torch.Tensor\n",
    "  ) -> torch.Tensor:\n",
    "    embeddings = self.embedding(tokens) * math.sqrt(self.dim_embeddings)\n",
    "    embeddings_pe = self.positional_encoding(embeddings)\n",
    "\n",
    "    output = embeddings_pe\n",
    "\n",
    "    for decoder in self.decoders:\n",
    "      output = decoder(output, decoder_padding_mask, context, encoder_padding_mask)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b60b8f-19b4-4ad6-8a08-803ada44b1c3",
   "metadata": {},
   "source": [
    "### 2.7. Transformer\n",
    "\n",
    "El _transformer_ se implementa siguiendo el _paper_ original.\n",
    "\n",
    "```\n",
    "Input ─> Encoder x N ─> Decoder x M ─> Linear ─> Output\n",
    "```\n",
    "\n",
    "La capa de _embedding_ es compartida por el _stack_ de _encoders_, el _stack_ de _decoders_, y su matriz de pesos con la capa lineal.\n",
    "\n",
    "Al entrenar el modelo con _batches_, cada secuencia del _batch_ puede tener una longitud distinta, por lo que se construye una matriz de _padding_ que enmascara los _tokens_ del final de cada secuencia para que tengan la misma longitud que la secuencia más larga.\n",
    "\n",
    "La salida del _transformer_ son los habitualmente llamados \"_logits_\" (valores de entrada a la función _SoftMax_), en vez de una secuencia de _tokens_. Esto facilita el entrenamiento, ya que habitualmente se utiliza la entropía cruzada como función de pérdida. Esta función aplica la función _SoftMax_, por lo que recibe directamente los _logits_ de la salida del modelo, sin necesidad de llamar a _SoftMax_ dentro del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52baa90-5b9f-4dbc-b488-6fca6658209f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    vocabulary_size: int,\n",
    "    dim_embeddings: int,\n",
    "    pad_token_id: int,\n",
    "    max_sequence_length: int,\n",
    "    num_encoders: int,\n",
    "    num_decoders: int,\n",
    "    num_heads: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pad_token_id = pad_token_id\n",
    "\n",
    "    self.embedding = nn.Embedding(\n",
    "      num_embeddings=vocabulary_size, embedding_dim=dim_embeddings, padding_idx=pad_token_id\n",
    "    )\n",
    "        \n",
    "    self.encoder_stack = EncoderStack(\n",
    "      num_encoders=num_encoders,\n",
    "      embedding=self.embedding,\n",
    "      dim_embeddings=dim_embeddings,\n",
    "      max_sequence_length=max_sequence_length,\n",
    "      num_heads=num_heads,\n",
    "      dim_feedforward=dim_feedforward,\n",
    "      dropout=dropout\n",
    "    )\n",
    "\n",
    "    self.decoder_stack = DecoderStack(\n",
    "      num_decoders=num_decoders,\n",
    "      embedding=self.embedding,\n",
    "      dim_embeddings=dim_embeddings,\n",
    "      max_sequence_length=max_sequence_length,\n",
    "      num_heads=num_heads,\n",
    "      dim_feedforward=dim_feedforward,\n",
    "      dropout=dropout\n",
    "    )\n",
    "      \n",
    "    self.linear = nn.Linear(in_features=dim_embeddings, out_features=vocabulary_size, bias=False)\n",
    "    self.linear.weight = self.embedding.weight\n",
    "\n",
    "  def forward(self, input_sequence: torch.Tensor, output_sequence: torch.Tensor) -> torch.Tensor:\n",
    "    encoder_padding_mask = (input_sequence == self.pad_token_id)\n",
    "    encoder_context = self.encoder_stack(input_sequence, encoder_padding_mask)\n",
    "\n",
    "    decoder_padding_mask = (output_sequence == self.pad_token_id)\n",
    "    decoder_output = self.decoder_stack(output_sequence, decoder_padding_mask, encoder_context, encoder_padding_mask)\n",
    "\n",
    "    logits = self.linear(decoder_output)\n",
    "\n",
    "    return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
